<!DOCTYPE html>


<html lang="Chinese">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> Frank’s blogs</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/FLogo.png" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/bkgPic.png" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">Frank’s blogs</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.staticfile.org/typed.js/2.0.12/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['Linux开发和工科生折腾集锦', '算法题总结', 'C/C++/python等开发'],
        startDelay: 0,
        typeSpeed: 100,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  
  
<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content">Linux编程、C/C++开发、算法题、各种工科生折腾开发集锦</div>
</div>


<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-ComputerNetwork"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/01/01/ComputerNetwork/"
    >Computer Network</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/01/01/ComputerNetwork/" class="article-date">
  <time datetime="2023-01-01T04:24:38.000Z" itemprop="datePublished">2023-01-01</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="计算机网络"><a href="#计算机网络" class="headerlink" title="计算机网络"></a>计算机网络</h1><h3 id="什么是三次握手-three-way-handshake-？"><a href="#什么是三次握手-three-way-handshake-？" class="headerlink" title="什么是三次握手 (three-way handshake)？"></a>什么是三次握手 (three-way handshake)？</h3><ul>
<li>第一次握手：Client将SYN置1，随机产生一个初始序列号seq发送给Server，进入SYN_SENT状态；</li>
<li>第二次握手：Server收到Client的SYN=1之后，知道客户端请求建立连接，将自己的SYN置1，ACK置1，产生一个acknowledge number=sequence number+1，并随机产生一个自己的初始序列号，发送给客户端；进入SYN_RCVD状态；</li>
<li>第三次握手：客户端检查acknowledge number是否为序列号+1，ACK是否为1，检查正确之后将自己的ACK置为1，产生一个acknowledge number=服务器发的序列号+1，发送给服务器；进入ESTABLISHED状态；服务器检查ACK为1和acknowledge number为序列号+1之后，也进入ESTABLISHED状态；完成三次握手，连接建立。</li>
<li>注意，ACK字段在正常传输过程中一般都是1，只要包含对对方发送的数据的确认序号的，ACK都是1，包括连接建立除了第一次握手之外的部分</li>
</ul>
<h5 id="TCP建立连接可以两次握手吗？为什么"><a href="#TCP建立连接可以两次握手吗？为什么" class="headerlink" title="TCP建立连接可以两次握手吗？为什么?"></a>TCP建立连接可以两次握手吗？为什么?</h5><p>不可以。有两个原因：</p>
<p>首先，可能会出现<strong>已失效的连接请求报文段又传到了服务器端</strong>。</p>
<blockquote>
<p>client 发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达 server。本来这是一个早已失效的报文段。但 server 收到此失效的连接请求报文段后，就误认为是 client 再次发出的一个新的连接请求。于是就向 client 发出确认报文段，同意建立连接。假设不采用 “三次握手”，那么只要 server 发出确认，新的连接就建立了。由于现在 client 并没有发出建立连接的请求，因此不会理睬 server 的确认，也不会向 server 发送数据。但 server 却以为新的运输连接已经建立，并一直等待 client 发来数据。这样，server 的很多资源就白白浪费掉了。采用 “三次握手” 的办法可以防止上述现象发生。例如刚才那种情况，client 不会向 server 的确认发出确认。server 由于收不到确认，就知道 client 并没有要求建立连接。</p>
</blockquote>
<p>其次，两次握手无法保证Client正确接收第二次握手的报文（Server无法确认Client是否收到），也无法保证Client和Server之间成功互换初始序列号。</p>
<p>还有就是两次握手会给SYN flood攻击提供机会。<br>扩展阅读： 什么是SYN攻击？<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/360479307">https://zhuanlan.zhihu.com/p/360479307</a></p>
<h5 id="可以采用四次握手吗？为什么？"><a href="#可以采用四次握手吗？为什么？" class="headerlink" title="可以采用四次握手吗？为什么？"></a>可以采用四次握手吗？为什么？</h5><p>可以。但是会降低传输的效率。</p>
<p>四次握手是指：第二次握手：Server只发送ACK和acknowledge number；而Server的SYN和初始序列号在第三次握手时发送；原来协议中的第三次握手变为第四次握手。出于优化目的，四次握手中的二、三可以合并。</p>
<h5 id="第三次握手中，如果客户端的ACK未送达服务器，会怎样？"><a href="#第三次握手中，如果客户端的ACK未送达服务器，会怎样？" class="headerlink" title="第三次握手中，如果客户端的ACK未送达服务器，会怎样？"></a>第三次握手中，如果客户端的ACK未送达服务器，会怎样？</h5><p>Server端：<br>由于Server没有收到ACK确认，因此会重发之前的SYN+ACK（默认重发五次，之后自动关闭连接进入CLOSED状态），Client收到后会重新传ACK给Server。</p>
<p>Client端，两种情况：  </p>
<ol>
<li>在Server进行超时重发的过程中，如果Client向服务器发送数据，数据头部的ACK是为1的，所以服务器收到数据之后会读取 ACK number，进入 establish 状态  </li>
<li>在Server进入CLOSED状态之后，如果Client向服务器发送数据，服务器会以RST包应答。</li>
</ol>
<h5 id="如果已经建立了连接，但客户端出现了故障怎么办？"><a href="#如果已经建立了连接，但客户端出现了故障怎么办？" class="headerlink" title="如果已经建立了连接，但客户端出现了故障怎么办？"></a>如果已经建立了连接，但客户端出现了故障怎么办？</h5><p>服务器每收到一次客户端的请求后都会重新复位一个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。</p>
<h5 id="初始序列号是什么？"><a href="#初始序列号是什么？" class="headerlink" title="初始序列号是什么？"></a>初始序列号是什么？</h5><p>TCP连接的一方A，随机选择一个32位的序列号（Sequence Number）作为发送数据的初始序列号（Initial Sequence Number，ISN），比如为1000，以该序列号为原点，对要传送的数据进行编号：1001、1002…三次握手时，把这个初始序列号传送给另一方B，以便在传输数据时，B可以确认什么样的数据编号是合法的；同时在进行数据传输时，A还可以确认B收到的每一个字节，如果A收到了B的确认编号（acknowledge number）是2001，就说明编号为1001-2000的数据已经被B成功接受。</p>
<h3 id="什么是四次挥手？"><a href="#什么是四次挥手？" class="headerlink" title="什么是四次挥手？"></a>什么是四次挥手？</h3><ul>
<li>第一次挥手：Client将FIN置为1，发送一个序列号seq给Server；进入FIN_WAIT_1状态；</li>
<li>第二次挥手：Server收到FIN之后，发送一个ACK=1，acknowledge number=收到的序列号+1；进入CLOSE_WAIT状态。此时客户端已经没有要发送的数据了，但仍可以接受服务器发来的数据。</li>
<li>第三次挥手：Server将FIN置1，发送一个序列号给Client；进入LAST_ACK状态；</li>
<li>第四次挥手：Client收到服务器的FIN后，进入TIME_WAIT状态；接着将ACK置1，发送一个acknowledge number=序列号+1给服务器；服务器收到后，确认acknowledge number后，变为CLOSED状态，不再向客户端发送数据。客户端等待2*MSL（报文段最长寿命）时间后，也进入CLOSED状态。完成四次挥手。</li>
</ul>
<h5 id="为什么不能把服务器发送的ACK和FIN合并起来，变成三次挥手（CLOSE-WAIT状态意义是什么）？"><a href="#为什么不能把服务器发送的ACK和FIN合并起来，变成三次挥手（CLOSE-WAIT状态意义是什么）？" class="headerlink" title="为什么不能把服务器发送的ACK和FIN合并起来，变成三次挥手（CLOSE_WAIT状态意义是什么）？"></a>为什么不能把服务器发送的ACK和FIN合并起来，变成三次挥手（CLOSE_WAIT状态意义是什么）？</h5><p>因为服务器收到客户端断开连接的请求时，可能还有一些数据没有发完，这时先回复ACK，表示接收到了断开连接的请求。等到数据发完之后再发FIN，断开服务器到客户端的数据传送。</p>
<h5 id="如果第二次挥手时服务器的ACK没有送达客户端，会怎样？"><a href="#如果第二次挥手时服务器的ACK没有送达客户端，会怎样？" class="headerlink" title="如果第二次挥手时服务器的ACK没有送达客户端，会怎样？"></a>如果第二次挥手时服务器的ACK没有送达客户端，会怎样？</h5><p>客户端没有收到ACK确认，会重新发送FIN请求。</p>
<h5 id="客户端TIME-WAIT状态的意义是什么？"><a href="#客户端TIME-WAIT状态的意义是什么？" class="headerlink" title="客户端TIME_WAIT状态的意义是什么？"></a>客户端TIME_WAIT状态的意义是什么？</h5><p>第四次挥手时，客户端发送给服务器的ACK有可能丢失，TIME_WAIT状态就是用来重发可能丢失的ACK报文。如果Server没有收到ACK，就会重发FIN，如果Client在2*MSL的时间内收到了FIN，就会重新发送ACK并再次等待2MSL，防止Server没有收到ACK而不断重发FIN。</p>
<p>MSL(Maximum Segment Lifetime)，指一个片段在网络中最大的存活时间，2MSL就是一个发送和一个回复所需的最大时间。如果直到2MSL，Client都没有再次收到FIN，那么Client推断ACK已经被成功接收，则结束TCP连接。</p>
<h3 id="TCP如何实现流量控制？"><a href="#TCP如何实现流量控制？" class="headerlink" title="TCP如何实现流量控制？"></a>TCP如何实现流量控制？</h3><p>使用滑动窗口协议实现流量控制。防止发送方发送速率太快，接收方缓存区不够导致溢出。接收方会维护一个接收窗口 receiver window（窗口大小单位是字节），接受窗口的大小是根据自己的资源情况动态调整的，在返回ACK时将接受窗口大小放在TCP报文中的窗口字段告知发送方。发送窗口的大小不能超过接受窗口的大小，只有当发送方发送并收到确认之后，才能将发送窗口右移。</p>
<p>发送窗口的上限为接受窗口和拥塞窗口中的较小值。接受窗口表明了接收方的接收能力，拥塞窗口表明了网络的传送能力。</p>
<h5 id="什么是零窗口（接收窗口为0时会怎样）？"><a href="#什么是零窗口（接收窗口为0时会怎样）？" class="headerlink" title="什么是零窗口（接收窗口为0时会怎样）？"></a>什么是零窗口（接收窗口为0时会怎样）？</h5><p>如果接收方没有能力接收数据，就会将接收窗口设置为0，这时发送方必须暂停发送数据，但是会启动一个持续计时器(persistence timer)，到期后发送一个大小为1字节的探测数据包，以查看接收窗口状态。如果接收方能够接收数据，就会在返回的报文中更新接收窗口大小，恢复数据传送。</p>
<h3 id="TCP的拥塞控制是怎么实现的？"><a href="#TCP的拥塞控制是怎么实现的？" class="headerlink" title="TCP的拥塞控制是怎么实现的？"></a>TCP的拥塞控制是怎么实现的？</h3><p>拥塞控制主要由四个算法组成：<strong>慢启动（Slow Start）、拥塞避免（Congestion voidance）、快重传 （Fast Retransmit）、快恢复（Fast Recovery）</strong></p>
<ol>
<li>慢启动：刚开始发送数据时，先把拥塞窗口（congestion window）设置为一个最大报文段MSS的数值，每收到一个新的确认报文之后，就把拥塞窗口加1个MSS。这样每经过一个传输轮次（或者说是每经过一个往返时间RTT），拥塞窗口的大小就会加倍</li>
</ol>
<ol start="2">
<li><p>拥塞避免：当拥塞窗口的大小达到慢开始门限(slow start threshold)时，开始执行拥塞避免算法，拥塞窗口大小不再指数增加，而是线性增加，即每经过一个传输轮次只增加1MSS.  </p>
<blockquote>
<p>无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（其根据就是没有收到确认），就要把慢开始门限ssthresh设置为出现拥塞时的发送方窗口值的一半（但不能小于2）。然后把拥塞窗口cwnd重新设置为1，执行慢开始算法。<strong>（这是不使用快重传的情况）</strong></p>
</blockquote>
</li>
<li><p>快重传：快重传要求接收方在收到一个失序的报文段后就立即发出<strong>重复确认</strong>（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时捎带确认。快重传算法规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。</p>
</li>
</ol>
<ol start="4">
<li>快恢复：当发送方连续收到三个重复确认时，就把慢开始门限减半，然后执行拥塞避免算法。不执行慢开始算法的原因：因为如果网络出现拥塞的话就不会收到好几个重复的确认，所以发送方认为现在网络可能没有出现拥塞。<br>也有的快重传是把开始时的拥塞窗口cwnd值再增大一点，即等于 ssthresh + 3*MSS 。这样做的理由是：既然发送方收到三个重复的确认，就表明有三个分组已经离开了网络。这三个分组不再消耗网络的资源而是停留在接收方的缓存中。可见现在网络中减少了三个分组。因此可以适当把拥塞窗口扩大些。</li>
</ol>
<h3 id="TCP如何最大利用带宽？"><a href="#TCP如何最大利用带宽？" class="headerlink" title="TCP如何最大利用带宽？"></a>TCP如何最大利用带宽？</h3><p>TCP速率受到三个因素影响</p>
<ul>
<li>窗口：即滑动窗口大小，见<a href="#TCP%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%B5%81%E9%87%8F%E6%8E%A7%E5%88%B6">TCP如何实现流量控制？</a></li>
<li>带宽：这里带宽是指单位时间内从发送端到接收端所能通过的“最高数据率”，是一种硬件限制。TCP发送端和接收端的数据传输数不可能超过两点间的带宽限制。发送端和接收端之间带宽取所通过线路的带宽最小值（如通过互联网连接）。</li>
<li>RTT：即Round Trip Time，表示从发送端到接收端的一去一回需要的时间，TCP在数据传输过程中会对RTT进行采样（即对发送的数据包及其ACK的时间差进行测量，并根据测量值更新RTT值），TCP根据得到的RTT值更新RTO值，即Retransmission TimeOut，就是重传间隔，发送端对每个发出的数据包进行计时，如果在RTO时间内没有收到所发出的数据包的对应ACK，则任务数据包丢失，将重传数据。一般RTO值都比采样得到的RTT值要大。</li>
</ul>
<summary>带宽时延乘积</summary>
带宽时延乘积=带宽*RTT，实际上等于发送端到接收端单向通道的数据容积的两倍，这里单向通道的数据容积可以这样来理解，单向通道看成是一条单行道马路，带宽就是马路的车道数，路上跑的汽车就是数据（不过这里所有汽车的速率都是一样的，且不会有人想超车，大家齐头并进），那么单向通道的数据容积就是这条单行道上摆满车，一共可以摆多少辆。带宽就是马路的车道数，带宽数乘以单向通道的数据容积就是路面上所能容纳的全部数据量。当路面上已经摆满的时候，就不能再往里面放了。


<p>设滑动窗口大小为<code>W</code> ， 发送端和接收端的带宽为<code>B</code></p>
<ul>
<li>前面已经说过了，TCP发送数据时受滑动窗口的限制，当TCP将滑动窗口中的数据都发出后，在收到第一个ACK之前，滑动窗口大小是0，不能再发送数据了，必须等待ACK包使滑动窗口移动。那么在理想情况下，ACK包应该在什么时候到达呢？显然，就是在数据发出后的RTT时间后，ACK包到达。这也就是说，现在在不考虑丢包和拥塞情况下，TCP在一个RTT时间内能发出的最大数据量为 <code>W</code>也就是一个滑动窗口的大小，所以不考虑带宽限制下，TCP一个时刻能达到的最大速度<code>V</code>是<code>窗口大小除以RTT</code></li>
</ul>
<p>现在再考虑带宽限制，前面说过当马路上摆满车的时候，就无法再往里放车了，同理，TCP发送端在<code>RTT除以2</code><br> 时间内，能往通道上放的最大数据量为<code>V乘RTT除以2</code>，通过带宽时延乘积得到的容积限制为<code>带宽B乘RTT除以2</code>。当<code>V乘RTT除以2</code>小于<code>带宽乘RTT除以2</code>时，单向通道容积不构成瓶颈，速率的限制主要来源于窗口大小限制。而当<code>V乘RTT除以2</code>大于<code>带宽乘RTT除以2</code>时，则就受到容积限制，即此时速率限制来源于带宽限制。</p>
<ul>
<li>因此，TCP的最大速率为<strong>滑动窗口的大小除以RTT的值和网络带宽中的最小值</strong></li>
<li>在我们平时生活中使用的宽带网络，ADSL等环境下，因为带宽都比较小，从而带宽与RTT的乘积也比较小，再加上网络情况比较复杂，拥塞情况比较常见，所以这些网络环境下，TCP速率的主要限制因素在于带宽，丢包率等。长肥管道一般不太常见，多见于一些单位使用的专线网络，在这些网络中速率的主要限制因素就是窗口大小了，这也是传统TCP在这些网络环境中不能充分利用带宽的原因所在（因为传统TCP的窗口大小是用2字节表示的，所以最大只有65535（不考虑窗口扩大选项）），除了专线网络外，随着网络硬件技术的发展，万兆交换机的出现，局域网中也可能会出现带宽时延乘积较大的情况。</li>
</ul>
<h3 id="TCP与UDP的区别"><a href="#TCP与UDP的区别" class="headerlink" title="TCP与UDP的区别"></a>TCP与UDP的区别</h3><ol>
<li>TCP是面向连接的，UDP是无连接的；</li>
</ol>
<summary>什么叫无连接？</summary>

<p>UDP发送数据之前不需要建立连接</p>
<ol start="2">
<li>TCP是可靠的，UDP不可靠；</li>
</ol>
<summary>什么叫不可靠？</summary>
UDP接收方收到报文后，不需要给出任何确认


<ol start="3">
<li>TCP只支持点对点通信，UDP支持一对一、一对多、多对一、多对多；</li>
<li>TCP是面向字节流的，UDP是面向报文的；</li>
</ol>
<summary>什么意思？</summary>

<p>面向字节流是指发送数据时以字节为单位，一个数据包可以拆分成若干组进行发送，而UDP一个报文只能一次发完。</p>
<ol start="5">
<li>TCP有拥塞控制机制，UDP没有。网络出现的拥塞不会使源主机的发送速率降低，这对某些实时应用是很重要的，比如媒体通信，游戏；</li>
<li>TCP首部开销（20字节）比UDP首部开销（8字节）要大</li>
<li>UDP 的主机不需要维持复杂的连接状态表</li>
</ol>
<h5 id="什么时候选择TCP，什么时候选UDP？"><a href="#什么时候选择TCP，什么时候选UDP？" class="headerlink" title="什么时候选择TCP，什么时候选UDP？"></a>什么时候选择TCP，什么时候选UDP？</h5><p>对某些实时性要求比较高的情况，选择UDP，比如游戏，媒体通信，实时视频流（直播），即使出现传输错误也可以容忍；其它大部分情况下，HTTP都是用TCP，因为要求传输的内容可靠，不出现丢失</p>
<h5 id="HTTP可以使用UDP吗？"><a href="#HTTP可以使用UDP吗？" class="headerlink" title="HTTP可以使用UDP吗？"></a>HTTP可以使用UDP吗？</h5><p>HTTP不可以使用UDP，HTTP需要基于可靠的传输协议，而UDP不可靠<br>注：<strong>http 3.0 使用udp实现</strong><br><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/HTTP/3">https://zh.wikipedia.org/wiki/HTTP/3</a></p>
<h5 id="面向连接和无连接的区别"><a href="#面向连接和无连接的区别" class="headerlink" title="面向连接和无连接的区别"></a>面向连接和无连接的区别</h5><p>无连接的网络服务（数据报服务） –   面向连接的网络服务（虚电路服务）</p>
<p>虚电路服务：首先建立连接，所有的数据包经过相同的路径，服务质量有较好的保证；</p>
<p>数据报服务：每个数据包含目的地址，数据路由相互独立（路径可能变化）；网络尽最大努力交付数据，但不保证不丢失、不保证先后顺序、不保证在时限内交付；网络发生拥塞时，可能会将一些分组丢弃；</p>
<h3 id="TCP如何保证传输的可靠性"><a href="#TCP如何保证传输的可靠性" class="headerlink" title="TCP如何保证传输的可靠性"></a>TCP如何保证传输的可靠性</h3><ol>
<li>数据包校验</li>
<li>对失序数据包重新排序（TCP报文具有序列号）</li>
<li>丢弃重复数据</li>
<li>应答机制：接收方收到数据之后，会发送一个确认（通常延迟几分之一秒）；</li>
<li>超时重发：发送方发出数据之后，启动一个定时器，超时未收到接收方的确认，则重新发送这个数据；</li>
<li>流量控制：确保接收端能够接收发送方的数据而不会缓冲区溢出</li>
</ol>
<h3 id="HTTP和HTTPS有什么区别？"><a href="#HTTP和HTTPS有什么区别？" class="headerlink" title="HTTP和HTTPS有什么区别？"></a>HTTP和HTTPS有什么区别？</h3><ol>
<li>端口不同：HTTP使用的是80端口，HTTPS使用443端口；</li>
<li>HTTP（超文本传输协议）信息是明文传输，HTTPS运行在SSL(Secure Socket Layer)之上，添加了加密和认证机制，更加安全；</li>
<li>HTTPS由于加密解密会带来更大的CPU和内存开销；</li>
<li>HTTPS通信需要证书，一般需要向证书颁发机构（CA）购买 </li>
</ol>
<h5 id="Https的连接过程？"><a href="#Https的连接过程？" class="headerlink" title="Https的连接过程？"></a>Https的连接过程？</h5><ol>
<li>客户端向服务器发送请求，同时发送客户端支持的一套加密规则（包括对称加密、非对称加密、摘要算法）；</li>
<li>服务器从中选出一组加密算法与HASH算法，并将自己的身份信息以证书的形式发回给浏览器。证书里面包含了网站地址，<strong>加密公钥</strong>（用于非对称加密），以及证书的颁发机构等信息（证书中的私钥只能用于服务器端进行解密）；</li>
<li>客户端验证服务器的合法性，包括：证书是否过期，CA 是否可靠，发行者证书的公钥能否正确解开服务器证书的“发行者的数字签名”，服务器证书上的域名是否和服务器的实际域名相匹配；</li>
<li>如果证书受信任，或者用户接收了不受信任的证书，浏览器会生成一个<strong>随机密钥</strong>（用于对称算法），并用服务器提供的公钥加密（采用非对称算法对密钥加密）；使用Hash算法对握手消息进行<strong>摘要</strong>计算，并对摘要使用之前产生的密钥加密（对称算法）；将加密后的随机密钥和摘要一起发送给服务器；</li>
<li>服务器使用自己的私钥解密，得到对称加密的密钥，用这个密钥解密出Hash摘要值，并验证握手消息是否一致；如果一致，服务器使用对称加密的密钥加密握手消息发给浏览器；</li>
<li>浏览器解密并验证摘要，若一致，则握手结束。之后的数据传送都使用对称加密的密钥进行加密</li>
</ol>
<p>总结：非对称加密算法用于在握手过程中加密生成的密码；对称加密算法用于对真正传输的数据进行加密；HASH算法用于验证数据的完整性。</p>
<h5 id="输入-www-baidu-com，怎么变成-https-www-baidu-com-的，怎么确定用HTTP还是HTTPS？"><a href="#输入-www-baidu-com，怎么变成-https-www-baidu-com-的，怎么确定用HTTP还是HTTPS？" class="headerlink" title="输入 www.baidu.com，怎么变成 https://www.baidu.com 的，怎么确定用HTTP还是HTTPS？"></a>输入 <a target="_blank" rel="noopener" href="http://www.baidu.com,怎么变成/">www.baidu.com，怎么变成</a> <a target="_blank" rel="noopener" href="https://www.baidu.com/">https://www.baidu.com</a> 的，怎么确定用HTTP还是HTTPS？</h5><p><a target="_blank" rel="noopener" href="https://www.sohu.com/a/136637876_487516">你访问的网站是如何自动切换到 HTTPS 的？</a><br>一种是原始的302跳转，服务器把所有的HTTp流量跳转到HTTPS。但这样有一个漏洞，就是中间人可能在第一次访问站点的时候就劫持。<br>解决方法是引入HSTS机制，用户浏览器在访问站点的时候强制使用HTTPS。</p>
<h5 id="HTTPS连接的时候，怎么确定收到的包是服务器发来的（中间人攻击）？"><a href="#HTTPS连接的时候，怎么确定收到的包是服务器发来的（中间人攻击）？" class="headerlink" title="HTTPS连接的时候，怎么确定收到的包是服务器发来的（中间人攻击）？"></a>HTTPS连接的时候，怎么确定收到的包是服务器发来的（中间人攻击）？</h5><p>1.验证域名、有效期等信息是否正确。证书上都有包含这些信息，比较容易完成验证；<br>2.判断证书来源是否合法。每份签发证书都可以根据验证链查找到对应的根证书，操作系统、浏览器会在本地存储权威机构的根证书，利用本地根证书可以对对应机构签发证书完成来源验证；<br>3.判断证书是否被篡改。需要与 CA 服务器进行校验；<br>4.判断证书是否已吊销。通过CRL（Certificate Revocation List 证书注销列表）和 OCSP（Online Certificate Status Protocol 在线证书状态协议）实现，其中 OCSP 可用于第3步中以减少与 CA 服务器的交互，提高验证效率</p>
<h5 id="什么是对称加密、非对称加密？区别是什么？"><a href="#什么是对称加密、非对称加密？区别是什么？" class="headerlink" title="什么是对称加密、非对称加密？区别是什么？"></a>什么是对称加密、非对称加密？区别是什么？</h5><ul>
<li>对称加密：加密和解密采用相同的密钥。如：DES、RC2、RC4</li>
<li>非对称加密：需要两个密钥：公钥和私钥。如果用公钥加密，需要用私钥才能解密。如：RSA</li>
<li>区别：对称加密速度更快，通常用于大量数据的加密；非对称加密安全性更高（不需要传送私钥）</li>
</ul>
<h5 id="数字签名、报文摘要的原理"><a href="#数字签名、报文摘要的原理" class="headerlink" title="数字签名、报文摘要的原理"></a>数字签名、报文摘要的原理</h5><ul>
<li>发送者A用私钥进行签名，接收者B用公钥验证签名。因为除A外没有人有私钥，所以B相信签名是来自A。A不可抵赖，B也不能伪造报文。</li>
<li>摘要算法:MD5、SHA</li>
</ul>
<h3 id="GET与POST的区别？"><a href="#GET与POST的区别？" class="headerlink" title="GET与POST的区别？"></a>GET与POST的区别？</h3><ol>
<li>GET是幂等的，即读取同一个资源，总是得到相同的数据，POST不是幂等的；</li>
<li>GET一般用于从服务器获取资源，而POST有可能改变服务器上的资源；</li>
<li>请求形式上：GET请求的数据附在URL之后，在HTTP请求头中；POST请求的数据在请求体中；</li>
<li>安全性：GET请求可被缓存、收藏、保留到历史记录，且其请求数据明文出现在URL中。POST的参数不会被保存，安全性相对较高；</li>
<li>GET只允许ASCII字符，POST对数据类型没有要求，也允许二进制数据；</li>
<li>GET的长度有限制（操作系统或者浏览器），而POST数据大小无限制</li>
</ol>
<h3 id="Session与Cookie的区别？"><a href="#Session与Cookie的区别？" class="headerlink" title="Session与Cookie的区别？"></a>Session与Cookie的区别？</h3><p>Session是服务器端保持状态的方案，Cookie是客户端保持状态的方案</p>
<p>Cookie保存在客户端本地，客户端请求服务器时会将Cookie一起提交；Session保存在服务端，通过检索Sessionid查看状态。保存Sessionid的方式可以采用Cookie，如果禁用了Cookie，可以使用URL重写机制（把会话ID保存在URL中）。</p>
<h3 id="从输入网址到获得页面的过程-越详细越好-？"><a href="#从输入网址到获得页面的过程-越详细越好-？" class="headerlink" title="从输入网址到获得页面的过程 (越详细越好)？"></a>从输入网址到获得页面的过程 (越详细越好)？</h3><ol>
<li>浏览器查询 DNS，获取域名对应的IP地址:具体过程包括浏览器搜索自身的DNS缓存、搜索操作系统的DNS缓存、读取本地的Host文件和向本地DNS服务器进行查询等。对于向本地DNS服务器进行查询，如果要查询的域名包含在本地配置区域资源中，则返回解析结果给客户机，完成域名解析(此解析具有权威性)；如果要查询的域名不由本地DNS服务器区域解析，但该服务器已缓存了此网址映射关系，则调用这个IP地址映射，完成域名解析（此解析不具有权威性）。如果本地域名服务器并未缓存该网址映射关系，那么将根据其设置发起递归查询或者迭代查询；</li>
<li>浏览器获得域名对应的IP地址以后，浏览器向服务器请求建立链接，发起三次握手；</li>
<li>TCP/IP链接建立起来后，浏览器向服务器发送HTTP请求；</li>
<li>服务器接收到这个请求，并根据路径参数映射到特定的请求处理器进行处理，并将处理结果及相应的视图返回给浏览器；</li>
<li>浏览器解析并渲染视图，若遇到对js文件、css文件及图片等静态资源的引用，则重复上述步骤并向服务器请求这些资源；</li>
<li>浏览器根据其请求到的资源、数据渲染页面，最终向用户呈现一个完整的页面。</li>
</ol>
<h3 id="HTTP请求有哪些常见状态码？"><a href="#HTTP请求有哪些常见状态码？" class="headerlink" title="HTTP请求有哪些常见状态码？"></a>HTTP请求有哪些常见状态码？</h3><ol>
<li>2xx状态码：操作成功。200 OK</li>
<li>3xx状态码：重定向。301 永久重定向；302暂时重定向</li>
<li>4xx状态码：客户端错误。400 Bad Request；401 Unauthorized；403 Forbidden；404 Not Found； </li>
<li>5xx状态码：服务端错误。500服务器内部错误；501服务不可用</li>
</ol>
<h3 id="什么是RIP-Routing-Information-Protocol-距离矢量路由协议-算法是什么？"><a href="#什么是RIP-Routing-Information-Protocol-距离矢量路由协议-算法是什么？" class="headerlink" title="什么是RIP (Routing Information Protocol, 距离矢量路由协议)? 算法是什么？"></a>什么是RIP (Routing Information Protocol, 距离矢量路由协议)? 算法是什么？</h3><p>每个路由器维护一张表，记录该路由器到其它网络的”跳数“，路由器到与其直接连接的网络的跳数是1，每多经过一个路由器跳数就加1；更新该表时和相邻路由器交换路由信息；路由器允许一个路径最多包含15个路由器，如果跳数为16，则不可达。交付数据报时优先选取距离最短的路径。</p>
<p>（PS：RIP是应用层协议：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/19645407">https://www.zhihu.com/question/19645407</a>）</p>
<summary>优缺点</summary>

<ul>
<li>实现简单，开销小</li>
<li>随着网络规模扩大开销也会增大；</li>
<li>最大距离为15，限制了网络的规模；</li>
<li>当网络出现故障时，要经过较长的时间才能将此信息传递到所有路由器</li>
</ul>
<h3 id="计算机网络体系结构"><a href="#计算机网络体系结构" class="headerlink" title="计算机网络体系结构"></a>计算机网络体系结构</h3><ul>
<li><p>Physical, Data Link, Network, Transport, Application</p>
</li>
<li><p>应用层：常见协议：</p>
<ul>
<li>FTP(21端口)：文件传输协议</li>
<li>SSH(22端口)：远程登陆</li>
<li>SMTP(25端口)：发送邮件</li>
<li>POP3(110端口)：接收邮件</li>
<li>HTTP(80端口)：超文本传输协议</li>
<li>DNS(53端口)：运行在UDP上，域名解析服务</li>
<li>RIP 运行在UDP上 </li>
</ul>
</li>
<li><p>传输层：TCP/UDP</p>
</li>
<li><p>网络层：IP、ARP、NAT、RIP…</p>
</li>
</ul>
<summary>路由器、交换机位于哪一层？</summary>

<ul>
<li>路由器网络层，根据IP地址进行寻址；</li>
<li>交换机数据链路层，根据MAC地址进行寻址</li>
</ul>
<h3 id="IP地址的分类？"><a href="#IP地址的分类？" class="headerlink" title="IP地址的分类？"></a>IP地址的分类？</h3><p>路由器仅根据网络号net-id来转发分组，当分组到达目的网络的路由器之后，再按照主机号host-id将分组交付给主机；同一网络上的所有主机的网络号相同。</p>
<h3 id="什么叫划分子网？"><a href="#什么叫划分子网？" class="headerlink" title="什么叫划分子网？"></a>什么叫划分子网？</h3><p>从主机号host-id借用若干个比特作为子网号subnet-id；子网掩码：网络号a和子网号都为1，主机号为0；数据报仍然先按照网络号找到目的网络，发送到路由器，路由器再按照网络号和子网号找到目的子网：将子网掩码与目标地址逐比特与操作，若结果为某个子网的网络地址，则送到该子网。</p>
<h3 id="什么是ARP协议-Address-Resolution-Protocol-？"><a href="#什么是ARP协议-Address-Resolution-Protocol-？" class="headerlink" title="什么是ARP协议 (Address Resolution Protocol)？"></a>什么是ARP协议 (Address Resolution Protocol)？</h3><p><strong>ARP协议完成了IP地址与物理地址的映射</strong>。每一个主机都设有一个 ARP 高速缓存，里面有<strong>所在的局域网</strong>上的各主机和路由器的 IP 地址到硬件地址的映射表。当源主机要发送数据包到目的主机时，会先检查自己的ARP高速缓存中有没有目的主机的MAC地址，如果有，就直接将数据包发到这个MAC地址，如果没有，就向<strong>所在的局域网</strong>发起一个ARP请求的广播包（在发送自己的 ARP 请求时，同时会带上自己的 IP 地址到硬件地址的映射），收到请求的主机检查自己的IP地址和目的主机的IP地址是否一致，如果一致，则先保存源主机的映射到自己的ARP缓存，然后给源主机发送一个ARP响应数据包。源主机收到响应数据包之后，先添加目的主机的IP地址与MAC地址的映射，再进行数据传送。如果源主机一直没有收到响应，表示ARP查询失败。</p>
<p>如果所要找的主机和源主机不在同一个局域网上，那么就要通过 ARP 找到一个位于本局域网上的某个路由器的硬件地址，然后把分组发送给这个路由器，让这个路由器把分组转发给下一个网络。剩下的工作就由下一个网络来做。</p>
<h3 id="什么是NAT-Network-Address-Translation-网络地址转换-？"><a href="#什么是NAT-Network-Address-Translation-网络地址转换-？" class="headerlink" title="什么是NAT (Network Address Translation, 网络地址转换)？"></a>什么是NAT (Network Address Translation, 网络地址转换)？</h3><p>用于解决内网中的主机要和因特网上的主机通信。由NAT路由器将主机的本地IP地址转换为全球IP地址，分为静态转换（转换得到的全球IP地址固定不变）和动态NAT转换。</p>
<h2 id="HTTP各版本"><a href="#HTTP各版本" class="headerlink" title="HTTP各版本"></a>HTTP各版本</h2><ul>
<li><p>HTTP 0.9版本</p>
<ul>
<li>HTTP协议的第一个版本，功能简单，<strong>已弃用</strong></li>
<li>仅<strong>支持纯文本数据的传输</strong>，虽然支持HTML，但是<strong>不支持图片插入</strong></li>
<li>仅支持<strong>GET请求方式</strong>，且不支持请求头</li>
<li>无状态，<strong>短连接。</strong>没有对用户状态的管理；<strong>每次请求建立一个TCP连接，响应之后关闭TCP连接。</strong></li>
</ul>
</li>
<li><p>HTTP 1.0版本</p>
<ul>
<li>支持<strong>POST、GET、HEAD</strong>三种方法</li>
<li><strong>支持长连接keep-alive</strong>（但<strong>默认还是使用短连接</strong>：浏览器每一次请求建立一次TCP连接，请求处理完毕之后断开）。</li>
<li>服务器不跟踪用户的行为也不记录用户过往请求。</li>
</ul>
</li>
<li><p>HTTP 1.1版本</p>
<ul>
<li>新增<strong>PUT、DELETE</strong>、CONNECT、TRACE、OPTIONS方法，是现今<strong>使用最多</strong>的版本。</li>
<li><strong>支持长连接</strong>，在一次TCP连接中可以发送多个请求或响应，<strong>且默认使用长连接</strong>。</li>
<li>支持宽带优化、断点续传。请求的对象部分数据，可以不必发送整个对象；文件上传下载支持续传。</li>
<li><strong>因为长连接产生的问题</strong>：<strong>队头阻塞。长连接中，发送请求和响应都是串行化的，</strong>前面的消息会造成后面的消息也阻塞。解决方法是创建多个TCP连接，这样就可以基本保证了可用性，浏览器<strong>默认的最大TCP连接数是6个</strong>。</li>
</ul>
</li>
<li><p>HTTP 2.0版本</p>
<ul>
<li><strong>二进制分帧，所有帧都是用二进制编码，节省了空间</strong></li>
<li>多路复用：<strong>HTTP 2.0中所有的连接都是持久化的。</strong>相比1.1版本可以不用维护更多的TCP连接，在处理并发请求的时候，可以将多个数据流中<strong>互不依赖的帧</strong>可以<strong>乱序发送</strong>，同时还支持<strong>优先级</strong>。接收方接收之后可以根据帧头部信息将帧组合起来。（解决了1.1版本中的队头阻塞问题）</li>
<li>头部压缩：1.1版本每次传输都需要传输一份首部，2.0让双方各自缓存一份首部字段表，达到更快传输的目标。</li>
</ul>
</li>
<li><p>HTTP 3.0版本</p>
<ul>
<li>基于UDP的<strong>QUIC多路复用</strong>：处理并发请求的时候, 且如果各个数据包互不依赖，那么就不会造成<strong>使用TCP带来的队头阻塞问题</strong>。</li>
<li>这个问题源头上是因为TCP连接，TCP连接的性质决定了重传会影响队后的数据发送，所以干脆选用UDP来解决这个方案。</li>
<li>0RRT建链：RRT表示Round-Trip Time，3.0可以实现0RRT建链。一般来说HTTPS协议要建立完整链接包括<strong>TCP握手</strong>和<strong>TLS握手</strong>，总计需要至少2-3个RTT，普通的HTTP协议也需要至少1个RTT才可以完成握手<strong>。基于UDP的QUIC协议可以在第一次发送包的时候直接发送业务数据</strong>。但是由于首次连接需要发送公钥数据，所以首次连接并不使用这一方法。</li>
</ul>
<blockquote>
<p>参考文档：[图解 | 为什么HTTP3.0要弃用TCP协议，而改用UDP协议？_涛哥聊Python-CSDN博客](</p>
</blockquote>
</li>
</ul>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/justloveyou_/article/details/78303617">面试/笔试第一弹 —— 计算机网络面试问题集锦</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43103529/article/details/120813469">【面试】计算机网络_雨下一整晚real的博客-CSDN博客</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/yjxsdzx/article/details/71937886">什么时候选TCP、UDP？</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/bad_sheep/article/details/6158676">TCP速率与窗口，带宽，RTT之间的关系</a></li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-python随记"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/12/24/python%E9%9A%8F%E8%AE%B0/"
    >python随记</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/12/24/python%E9%9A%8F%E8%AE%B0/" class="article-date">
  <time datetime="2022-12-24T05:53:12.000Z" itemprop="datePublished">2022-12-24</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="matplotlib"><a href="#matplotlib" class="headerlink" title="matplotlib"></a>matplotlib</h2><ul>
<li>基本框架<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 指定默认字体</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 解决保存图像时符号-显示为方块的2问题</span></span><br><span class="line">plt.plot(xs, yx, label=<span class="string">&quot;x随迭代次数的变化&quot;</span>)</span><br><span class="line">plt.plot(xs, yy, label=<span class="string">&quot;y随迭代次数的变化&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">&quot;迭代次数&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;x以及f(x)的数值&quot;</span>)</span><br><span class="line">plt.title(<span class="string">r&quot;$x_0$ = 1&quot;</span>)</span><br><span class="line">plt.show()  </span><br></pre></td></tr></table></figure></li>
<li>使用latex语法：<code>r&quot;$&lt;latex语法&gt;$&quot;</code><h2 id="pycharm配置项目的python解释器"><a href="#pycharm配置项目的python解释器" class="headerlink" title="pycharm配置项目的python解释器"></a>pycharm配置项目的python解释器</h2></li>
<li>设置下面找到项目然后更改解释器<ul>
<li><img src="/imgs/73243950c62d71fd56e29802053af5f4ce772fb1ac73bfc9098e862fdd88ee36.png" alt="picture 1">  </li>
</ul>
</li>
<li><strong>注意单独更改此处没有用</strong><ul>
<li><img src="/imgs/ae6810ecdb55ecf83bab089970463402519d5ed979d75e1d7005bed717d6c11d.png" alt="picture 2">  </li>
</ul>
</li>
<li>python得到当前路径<ul>
<li><code>os.getcwd().replace(&#39;\\&#39;,&#39;/&#39;)</code>此处将<code>/</code>替换为了<code>\\</code>，实际上是单斜线<h3 id="数组切片"><a href="#数组切片" class="headerlink" title="数组切片"></a>数组切片</h3></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42782150/article/details/127014616">参考链接</a></li>
<li>注意<code>数组[begin:end]</code>切到的是begin到<strong>end-1</strong>的内容<h3 id="重启程序"><a href="#重启程序" class="headerlink" title="重启程序"></a>重启程序</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取当前解释器路径</span></span><br><span class="line">p = sys.executable</span><br><span class="line"><span class="comment"># 启动新程序(解释器路径, 当前程序)</span></span><br><span class="line">os.execl(p, p, *sys.argv)</span><br><span class="line"><span class="comment"># 关闭当前程序</span></span><br><span class="line">sys.exit()</span><br></pre></td></tr></table></figure>
<h3 id="pycharm内存不足"><a href="#pycharm内存不足" class="headerlink" title="pycharm内存不足"></a>pycharm内存不足</h3></li>
<li><a target="_blank" rel="noopener" href="https://www.ycpai.cn/python/dwqRcLg8.html">教程</a></li>
<li><img src="/imgs/66393c1b6f785f70b6cfa70b8b83a6f8705de4000772b5425184113d7106cb15.png" alt="picture 3">  </li>
</ul>
<h3 id="import-numpy的时候出错"><a href="#import-numpy的时候出错" class="headerlink" title="import numpy的时候出错"></a>import numpy的时候出错</h3><ul>
<li>有时候在一些<strong>非x86</strong>的设备上使用numpy会报错 <code>Illegal instruction (core dumped)</code></li>
<li><img src="/imgs/e8f026142d2e9e89d431c908409bd83cd625957400511b4df2b0ee781c22d5da.png" alt="picture 3">  </li>
<li>此时需要设置环境变量<code>~/.bashrc</code>追加一句<code>export OPENBLAS_CORETYPE=ARMV8</code>然后<code>source ~/.bashrc</code></li>
<li>此时不再报错<ul>
<li><img src="/imgs/78f122d9e620fc70c253acd0eb657b711ecd8026371a6ef3c360dd744739cc03.png" alt="picture 4">  </li>
</ul>
</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/matplotlib/" rel="tag">matplotlib</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-Unity学习笔记"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/12/21/Unity%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"
    >Unity学习笔记</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/12/21/Unity%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="article-date">
  <time datetime="2022-12-20T16:21:08.000Z" itemprop="datePublished">2022-12-21</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Unity学习笔记"><a href="#Unity学习笔记" class="headerlink" title="Unity学习笔记"></a>Unity学习笔记</h1><p><u><strong>先安装hub再安装Unity！！！</strong></u></p>
<h2 id="环境变量配置（建议用VS一步到位，VSCode配置环境太麻烦）"><a href="#环境变量配置（建议用VS一步到位，VSCode配置环境太麻烦）" class="headerlink" title="环境变量配置（建议用VS一步到位，VSCode配置环境太麻烦）"></a>环境变量配置（建议用VS一步到位，VSCode配置环境太麻烦）</h2><ul>
<li>右键文件管理器-&gt;属性-&gt;<img src="/imgs/7a45dd2b4ae080fe76c4387e1d39b34498f2d3b18611f42a1274bc17a4d2e25c.png" alt="picture 5">  </li>
<li>然后环境变量，添加<img src="/imgs/047fbdff8062f0b8d0fe2806b51cccd884cadc4c8be1c8fb6cddd968ea217925.png" alt="picture 6">  看这俩有没有，然后试一下命令行输入<code>dotnet --info</code></li>
</ul>
<h2 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV16W411D7sC?p=14&vd_source=2ad101445b39a9dac0437fdcd408895e">入门教程</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://docs.unity3d.com/2022.1/Documentation/ScriptReference/">官方文档</a></p>
<h2 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h2></li>
<li><p>用将对象拖拽到这个位置<img src="/imgs/fd77db333d98e6cb810a31c94a1629c8359b6da349617469610875b303c9798a.png" alt="picture 4"><br>的方式给<code>public</code>对象赋值，包括<code>GameObject</code>和<code>RigidBody</code>，这两个得<u>分开赋值</u></p>
</li>
<li><p><code>RigidBody</code>的<code>AddForce</code>不能开启<img src="/imgs/fdedeaddbfac4dea740aae258bcd25e49c578e1b4a6cc61b9238483bb50d35d8.png" alt="picture 2">  </p>
</li>
<li><p>刚体的<a target="_blank" rel="noopener" href="https://blog.csdn.net/LOVE_Me__/article/details/125851048">教程</a></p>
</li>
<li><p>is Kinematic的<a target="_blank" rel="noopener" href="https://blog.csdn.net/iiiiiiimp/article/details/126822387">教程</a></p>
<ul>
<li>isKinematic<strong>不会对碰撞和力做出反应</strong>，不受物理系统影响，但依然<strong>会对其他刚体产生物理影响</strong>（比如可以<strong>阻挡</strong>其他刚体）。</li>
<li>isKinematic只能<u>在脚本中修改物体的Transform属性来移动</u>。</li>
<li>用在经常需要移动等变化物理状态的碰撞体上。一个刚体碰撞体，可以随时开启或关闭Is Kinematic选项，不会像静态碰撞体的enabled开启或关闭那样引起物理系统的问题。</li>
</ul>
</li>
<li><p>给一个物理系统的刚体添加一个瞬时的速度的方法</p>
<ul>
<li><code>wbRd.AddForce(new Vector3(x, y, z) * 1.0f, ForceMode.Impulse);</code></li>
<li>或者ForceMode设置为<code>VelocityChange</code>，可以直接改变速度，类似于碰撞的效果</li>
</ul>
</li>
<li><p>获取时间用<code>Time</code>类，unity有支持</p>
</li>
<li><p>复位一个场景用<code>SceneManager.LoadScene(index);</code>，index是这个scene在最终的序列里拍第几个，从0开始</p>
</li>
<li><p>一个<code>Vector3.normalized</code>给出同方向的一个单位向量</p>
</li>
<li><p><code>transform.LookAt(transform)</code>是让当前对象的z轴指向目标对象（z轴是相机的拍照方向）</p>
</li>
<li><p>不规则物体生成碰撞体：<img src="/imgs/d2d0ca79325ddcc42b7949239c75d296bb0b0b5bdd12eb91330d0a4f2f44c5e6.png" alt="picture 3">  </p>
</li>
<li><p>数学计算用<code>Mathf</code>对象下面的操作函数，其中的三角函数是角度制（0-360°）的</p>
</li>
<li><p>鼠标位置用<code>Input.mousePosition</code>得到一个Vector2</p>
</li>
<li><p>键盘用<code>Input.GetKeyDown(KeyCode.按键名)</code>或者其他，可以查手册，KeyCode包含的内容也查手册</p>
</li>
<li><p><code>Transform.translate()</code>函数可以指定运动的坐标系是自身的还是世界的</p>
</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Unity/" rel="tag">Unity</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-机器学习知识帖"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/10/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E5%B8%96/"
    >机器学习知识帖</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E5%B8%96/" class="article-date">
  <time datetime="2022-10-31T07:45:40.000Z" itemprop="datePublished">2022-10-31</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33992985">梯度下降法</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34842727">正规方程法(最小二乘法)</a></p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2></li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28408516">逻辑回归</a></p>
<h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2></li>
<li><p>在逻辑回归中，最常用的是代价函数是交叉熵(Cross Entropy)，交叉熵是一个常见的代价函数，在神经网络中也会用到。下面是《神经网络与深度学习》一书对交叉熵的解释：</p>
</li>
<li><p>交叉熵是对「出乎意料」（译者注：原文使用suprise）的度量。神经元的目标是去计算函数x→y=y(x)。但是我们让它取而代之计算函数x→a=a(x)。假设我们把a当作y等于1的概率，1−a是y等于0的概率。那么，交叉熵衡量的是我们在知道y的真实值时的平均「出乎意料」程度。当输出是我们期望的值，我们的「出乎意料」程度比较低；当输出不是我们期望的，我们的「出乎意料」程度就比较高。</p>
</li>
<li><p>在1948年，克劳德·艾尔伍德·香农将热力学的熵，引入到信息论，因此它又被称为香农熵(Shannon Entropy)，它是香农信息量(Shannon Information Content, SIC)的期望。香农信息量用来度量不确定性的大小：一个事件的香农信息量等于0，表示该事件的发生不会给我们提供任何新的信息，例如确定性的事件，发生的概率是1，发生了也不会引起任何惊讶；当不可能事件发生时，香农信息量为无穷大，这表示给我们提供了无穷多的新信息，并且使我们无限的惊讶</p>
</li>
<li><p>[交叉熵](<a target="_blank" rel="noopener" href="https://blog.csdn.net/rtygbwwwerr/article/details/50778098">https://blog.csdn.net/rtygbwwwerr/article/details/50778098</a></p>
</li>
<li><p><img src="/imgs/320549d4337620d61dd7b081632aa72e223e40cb5484ebaedd5756de3236de03.png" alt="picture 1">  </p>
<ul>
<li>上述式子是熵的期望（不同情况的熵的加权和）</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28415991">代价函数</a></p>
<h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2></li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/v_july_v/article/details/7624837">SVM原理</a></p>
<h2 id="KMeans"><a href="#KMeans" class="headerlink" title="KMeans"></a>KMeans</h2></li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/78798251?utm_source=qq">KMeans</a></p>
<h2 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h2></li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/110807201">Dueling DQN</a></p>
<h2 id="文章链接"><a href="#文章链接" class="headerlink" title="文章链接"></a>文章链接</h2></li>
<li><p><a target="_blank" rel="noopener" href="https://openai.com/blog/vpt/">openai用强化学习玩我的世界</a></p>
</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-贝叶斯回归"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/10/16/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9B%9E%E5%BD%92/"
    >贝叶斯回归</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/16/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9B%9E%E5%BD%92/" class="article-date">
  <time datetime="2022-10-16T03:20:52.000Z" itemprop="datePublished">2022-10-16</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/436569049">参考链接</a><br><a target="_blank" rel="noopener" href="https://www.zhihu.com/people/tao-tie-wen-10/posts">总链接</a></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-PPO调参实录"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/10/13/PPO%E8%B0%83%E5%8F%82%E5%AE%9E%E5%BD%95/"
    >PPO调参实录</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/13/PPO%E8%B0%83%E5%8F%82%E5%AE%9E%E5%BD%95/" class="article-date">
  <time datetime="2022-10-13T10:54:17.000Z" itemprop="datePublished">2022-10-13</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="PPO调参实录"><a href="#PPO调参实录" class="headerlink" title="PPO调参实录"></a>PPO调参实录</h1><h2 id="采用甄别reward的replay-buffer"><a href="#采用甄别reward的replay-buffer" class="headerlink" title="采用甄别reward的replay buffer"></a>采用甄别reward的replay buffer</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">store</span>(<span class="params">self, transition</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(self.buffer)&gt;=self.buffer_capacity:</span><br><span class="line">        tmp = np.random.randint(low=<span class="number">0</span>, high=<span class="built_in">len</span>(self.buffer))</span><br><span class="line">        <span class="keyword">if</span> self.buffer[tmp].r &gt; transition.r:</span><br><span class="line">            <span class="keyword">if</span> np.random.randint(low=<span class="number">0</span>, high=<span class="number">2</span>) == <span class="number">1</span>:  <span class="comment"># 一半的概率 替换的时候会保留reward更高的</span></span><br><span class="line">                self.buffer[tmp] = transition</span><br><span class="line">        self.buffer[tmp] = transition</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    self.counter += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> self.counter % self.buffer_capacity == <span class="number">0</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>结果</p>
<ul>
<li><img src="/imgs/e94f577e49117f5b41cc7faa4f930abd7e60400dbb265f1cc2530435a3534e2f.png" alt="image.png 1">  </li>
</ul>
</li>
<li><p>甄别的比率降到1/3之后</p>
<ul>
<li><img src="/imgs/71d730a255173dfdc717d546f3cb9815b2b53c00e33de147e389c889754c1d03.png" alt="picture 4">  </li>
</ul>
</li>
</ul>
<h2 id="优化（22年11月13日）的网络"><a href="#优化（22年11月13日）的网络" class="headerlink" title="优化（22年11月13日）的网络"></a>优化（22年11月13日）的网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> Normal</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.sampler <span class="keyword">import</span> BatchSampler, SubsetRandomSampler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">torch.set_num_threads(<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;Solve the Pendulum-v0 with PPO&#x27;</span>)</span><br><span class="line">parser.add_argument(</span><br><span class="line">    <span class="string">&#x27;--gamma&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.9</span>, metavar=<span class="string">&#x27;G&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;discount factor (default: 0.9)&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>, metavar=<span class="string">&#x27;N&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;random seed (default: 0)&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--render&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;render the environment&#x27;</span>)</span><br><span class="line">parser.add_argument(</span><br><span class="line">    <span class="string">&#x27;--log-interval&#x27;</span>,</span><br><span class="line">    <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">    default=<span class="number">10</span>,</span><br><span class="line">    metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">    <span class="built_in">help</span>=<span class="string">&#x27;interval between training status logs (default: 10)&#x27;</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">torch.manual_seed(args.seed)</span><br><span class="line"></span><br><span class="line">TrainingRecord = namedtuple(<span class="string">&#x27;TrainingRecord&#x27;</span>, [<span class="string">&#x27;ep&#x27;</span>, <span class="string">&#x27;reward&#x27;</span>])</span><br><span class="line">Transition = namedtuple(<span class="string">&#x27;Transition&#x27;</span>, [<span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;a_log_p&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s_&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ActorNet, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(<span class="number">3</span>, <span class="number">200</span>)</span><br><span class="line">        self.mu_head = nn.Linear(<span class="number">200</span>, <span class="number">1</span>)</span><br><span class="line">        self.sigma_head = nn.Linear(<span class="number">200</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.fc(x))</span><br><span class="line">        mu = <span class="number">2.0</span> * torch.tanh(self.mu_head(x))</span><br><span class="line">        sigma = F.softplus(self.sigma_head(x))</span><br><span class="line">        <span class="keyword">return</span> (mu, sigma)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CriticNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CriticNet, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(<span class="number">3</span>, <span class="number">200</span>)</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">200</span>, <span class="number">200</span>)</span><br><span class="line">        self.v_head = nn.Linear(<span class="number">200</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.fc(x))</span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        state_value = self.v_head(x)</span><br><span class="line">        <span class="keyword">return</span> state_value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>():</span></span><br><span class="line">    clip_param = <span class="number">0.1</span></span><br><span class="line">    max_grad_norm = <span class="number">0.5</span></span><br><span class="line">    ppo_epoch = <span class="number">5</span></span><br><span class="line">    buffer_capacity, batch_size = <span class="number">1000</span>, <span class="number">32</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.training_step = <span class="number">0</span></span><br><span class="line">        self.anet = ActorNet().<span class="built_in">float</span>()</span><br><span class="line">        self.cnet = CriticNet().<span class="built_in">float</span>()</span><br><span class="line">        self.buffer = []</span><br><span class="line">        self.counter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        self.optimizer_a = optim.Adam(self.anet.parameters(), lr=<span class="number">4e-5</span>)</span><br><span class="line">        self.optimizer_c = optim.Adam(self.cnet.parameters(), lr=<span class="number">6e-5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        state = torch.from_numpy(state).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            (mu, sigma) = self.anet(state)</span><br><span class="line">        dist = Normal(mu, sigma)</span><br><span class="line">        action = dist.sample()</span><br><span class="line">        action_log_prob = dist.log_prob(action)</span><br><span class="line">        action = action.clamp(-<span class="number">2.0</span>, <span class="number">2.0</span>)</span><br><span class="line">        <span class="keyword">return</span> action.item(), action_log_prob.item()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_value</span>(<span class="params">self, state</span>):</span></span><br><span class="line"></span><br><span class="line">        state = torch.from_numpy(state).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            state_value = self.cnet(state)</span><br><span class="line">        <span class="keyword">return</span> state_value.item()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_param</span>(<span class="params">self</span>):</span></span><br><span class="line">        torch.save(self.anet.state_dict(), <span class="string">&#x27;param/ppo_anet_params.pkl&#x27;</span>)</span><br><span class="line">        torch.save(self.cnet.state_dict(), <span class="string">&#x27;param/ppo_cnet_params.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store</span>(<span class="params">self, transition</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.buffer)&gt;=self.buffer_capacity:</span><br><span class="line">            tmp = np.random.randint(low=<span class="number">0</span>, high=<span class="built_in">len</span>(self.buffer))</span><br><span class="line">            <span class="keyword">if</span> self.buffer[tmp].r &gt; transition.r:</span><br><span class="line">                <span class="keyword">if</span> np.random.randint(low=<span class="number">0</span>, high=<span class="number">3</span>) == <span class="number">1</span>:  <span class="comment"># 一半的概率 替换的时候会保留reward更高的</span></span><br><span class="line">                    self.buffer[tmp] = transition</span><br><span class="line">            self.buffer[tmp] = transition</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.buffer.append(transition)</span><br><span class="line">        self.counter += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.counter % self.buffer_capacity == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.training_step += <span class="number">1</span></span><br><span class="line">        self.counter = <span class="number">1</span></span><br><span class="line">        s = torch.tensor([t.s <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        a = torch.tensor([t.a <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        r = torch.tensor([t.r <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        s_ = torch.tensor([t.s_ <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">        old_action_log_probs = torch.tensor(</span><br><span class="line">            [t.a_log_p <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        r = (r - r.mean()) / (r.std() + <span class="number">1e-5</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            target_v = r + args.gamma * self.cnet(s_)</span><br><span class="line"></span><br><span class="line">        adv = (target_v - self.cnet(s)).detach()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.ppo_epoch):</span><br><span class="line">            <span class="comment"># indexArray = []</span></span><br><span class="line">            <span class="comment"># dist = Normal(self.buffer_capacity, self.buffer_capacity / 4)</span></span><br><span class="line">            <span class="comment"># sampleList = []</span></span><br><span class="line">            <span class="comment"># cntArray = [0 for i in range(self.buffer_capacity)]</span></span><br><span class="line">            <span class="comment"># # print(cntArray)</span></span><br><span class="line">            <span class="comment"># dist = Normal(self.batch_size*2/3, self.batch_size)</span></span><br><span class="line">            <span class="comment"># for i in range(self.batch_size):</span></span><br><span class="line">            <span class="comment">#     cnt = 0</span></span><br><span class="line">            <span class="comment">#     samples = []</span></span><br><span class="line">            <span class="comment">#     while cnt &lt; self.batch_size:</span></span><br><span class="line">            <span class="comment">#         # print(dist.sample())</span></span><br><span class="line">            <span class="comment">#         # print(cnt)</span></span><br><span class="line">            <span class="comment">#         tmp = round(float(dist.sample()))</span></span><br><span class="line">            <span class="comment">#         if 0 &lt;= tmp and tmp &lt; self.buffer_capacity:</span></span><br><span class="line">            <span class="comment">#             if not tmp in samples:</span></span><br><span class="line">            <span class="comment">#                 if not cntArray[tmp]&gt;4:</span></span><br><span class="line">            <span class="comment">#                     cnt += 1</span></span><br><span class="line">            <span class="comment">#                     samples.append(tmp)</span></span><br><span class="line">            <span class="comment">#                     cntArray[tmp]+=1</span></span><br><span class="line">            <span class="comment">#     sampleList.append(samples)</span></span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> BatchSampler(</span><br><span class="line">                    SubsetRandomSampler(<span class="built_in">range</span>(self.buffer_capacity)), self.batch_size, <span class="literal">False</span>):</span><br><span class="line">            <span class="comment"># for index in sampleList:</span></span><br><span class="line">                <span class="comment"># print(len(index))</span></span><br><span class="line">                (mu, sigma) = self.anet(s[index])</span><br><span class="line">                <span class="comment"># print(mu)</span></span><br><span class="line">                dist = Normal(mu, sigma)</span><br><span class="line">                action_log_probs = dist.log_prob(a[index])</span><br><span class="line">                ratio = torch.exp(action_log_probs - old_action_log_probs[index])</span><br><span class="line"></span><br><span class="line">                surr1 = ratio * adv[index]</span><br><span class="line">                surr2 = torch.clamp(ratio, <span class="number">1.0</span> - self.clip_param,</span><br><span class="line">                                    <span class="number">1.0</span> + self.clip_param) * adv[index]</span><br><span class="line">                action_loss = -torch.<span class="built_in">min</span>(surr1, surr2).mean()</span><br><span class="line"></span><br><span class="line">                self.optimizer_a.zero_grad()</span><br><span class="line">                action_loss.backward()</span><br><span class="line">                nn.utils.clip_grad_norm_(self.anet.parameters(), self.max_grad_norm)</span><br><span class="line">                self.optimizer_a.step()</span><br><span class="line"></span><br><span class="line">                value_loss = F.smooth_l1_loss(self.cnet(s[index]), target_v[index])</span><br><span class="line">                self.optimizer_c.zero_grad()</span><br><span class="line">                value_loss.backward()</span><br><span class="line">                nn.utils.clip_grad_norm_(self.cnet.parameters(), self.max_grad_norm)</span><br><span class="line">                self.optimizer_c.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># del self.buffer[:]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">flagVar = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">pendulumGoal = [<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rewardFunc</span>(<span class="params">goal, state, absMax</span>):</span></span><br><span class="line">    tmp = -(np.power(state[<span class="number">0</span>] - goal[<span class="number">0</span>], <span class="number">2</span>) + <span class="number">1</span>*np.power(state[<span class="number">1</span>] - goal[<span class="number">1</span>], <span class="number">2</span>) + <span class="number">0.1</span> * np.power(state[<span class="number">2</span>] - goal[<span class="number">2</span>], <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> (tmp + absMax)/absMax</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="keyword">global</span> flagVar</span><br><span class="line">    <span class="comment"># env = gym.make(&#x27;Pendulum-v1&#x27;, render_mode=&quot;human&quot;)</span></span><br><span class="line">    env = gym.make(<span class="string">&#x27;Pendulum-v1&#x27;</span>)</span><br><span class="line">    <span class="comment"># env.seed(args.seed)</span></span><br><span class="line">    <span class="comment"># env.render()</span></span><br><span class="line">    agent = Agent()</span><br><span class="line"></span><br><span class="line">    training_records = []</span><br><span class="line">    running_reward = -<span class="number">1000</span></span><br><span class="line">    <span class="comment"># state = env.reset()</span></span><br><span class="line">    TRAIN_EPISODE = <span class="number">1000</span></span><br><span class="line">    EPISODE_LENGTH = <span class="number">200</span></span><br><span class="line">    <span class="keyword">for</span> i_ep <span class="keyword">in</span> <span class="built_in">range</span>(TRAIN_EPISODE):</span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">        state = np.array(env.reset()[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># if i_ep % 100 == 0:</span></span><br><span class="line">        <span class="comment">#     print(&#x27;!&#x27;)</span></span><br><span class="line">        <span class="comment">#     env.render()</span></span><br><span class="line">        <span class="comment"># print(type(state))</span></span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">if</span> i_ep == <span class="number">0</span>:</span><br><span class="line">            firstY = []</span><br><span class="line">        <span class="keyword">if</span> i_ep == TRAIN_EPISODE-<span class="number">1</span>:</span><br><span class="line">            lastX = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(EPISODE_LENGTH)]</span><br><span class="line">            lastY = []</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(EPISODE_LENGTH):</span><br><span class="line">            action, action_log_prob = agent.select_action(state)</span><br><span class="line">            <span class="comment"># if i_ep%100 == 0:</span></span><br><span class="line">                <span class="comment"># print(&#x27;!&#x27;)</span></span><br><span class="line">                <span class="comment"># env.render()</span></span><br><span class="line">            envRet = env.step([action])</span><br><span class="line">            <span class="comment"># print(envRet)</span></span><br><span class="line">            state_, reward, done, _, _ = envRet</span><br><span class="line">            <span class="keyword">if</span> i_ep == TRAIN_EPISODE-<span class="number">1</span>:</span><br><span class="line">                lastY.append(math.acos(state_[<span class="number">0</span>]))</span><br><span class="line">                <span class="comment"># lastY.append(state_[0])</span></span><br><span class="line">            <span class="keyword">elif</span> i_ep == <span class="number">0</span>:</span><br><span class="line">                firstY.append(math.acos(state_[<span class="number">0</span>]))</span><br><span class="line">                <span class="comment"># firstY.append(state_[0])</span></span><br><span class="line">            <span class="comment"># print(state_)</span></span><br><span class="line">            <span class="comment"># if args.render:</span></span><br><span class="line">            <span class="comment">#     env.render()</span></span><br><span class="line">            <span class="keyword">if</span> agent.store(Transition(state, action, action_log_prob, (reward + <span class="number">8</span>) / <span class="number">8</span>, state_)):</span><br><span class="line">                agent.update()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># reward = rewardFunc(pendulumGoal, state_, 8)</span></span><br><span class="line">            <span class="comment"># if agent.store(Transition(state, action, action_log_prob, reward, state_)):</span></span><br><span class="line">            <span class="comment">#     agent.update()</span></span><br><span class="line"></span><br><span class="line">            score += reward</span><br><span class="line">            state = state_</span><br><span class="line"></span><br><span class="line">        running_reward = running_reward * <span class="number">0.85</span> + score * <span class="number">0.1</span></span><br><span class="line">        training_records.append(TrainingRecord(i_ep, running_reward))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i_ep % args.log_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Ep &#123;&#125;\tMoving average score: &#123;:.2f&#125;\t&#x27;</span>.<span class="built_in">format</span>(i_ep, running_reward))</span><br><span class="line">            time_ = time.time()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;time used is &#123;:.5f&#125;&quot;</span>.<span class="built_in">format</span>(time_ - start))</span><br><span class="line">            <span class="comment"># start = time_</span></span><br><span class="line">        <span class="comment"># if running_reward &gt; -200:</span></span><br><span class="line">        <span class="comment">#     print(&quot;Solved! Moving average score is now &#123;&#125;!&quot;.format(running_reward))</span></span><br><span class="line">        <span class="comment">#     env.close()</span></span><br><span class="line">        <span class="comment">#     agent.save_param()</span></span><br><span class="line">        <span class="comment">#     with open(&#x27;log/ppo_training_records.pkl&#x27;, &#x27;wb&#x27;) as f:</span></span><br><span class="line">        <span class="comment">#         pickle.dump(training_records, f)</span></span><br><span class="line">        <span class="comment">#     break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># if running_reward &gt; -850 and not flagVar:  # 减小学习率防止波动</span></span><br><span class="line">        <span class="comment">#     flagVar = True</span></span><br><span class="line">        <span class="comment">#     print(&quot;-------------------------------&quot;)</span></span><br><span class="line">        <span class="comment">#     agent.optimizer_a = optim.Adam(agent.anet.parameters(), lr=1e-5)</span></span><br><span class="line">        <span class="comment">#     agent.optimizer_c = optim.Adam(agent.cnet.parameters(), lr=2e-5)</span></span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">    plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 指定默认字体</span></span><br><span class="line">    plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 解决保存图像时符号-显示为方块的2问题</span></span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    plt.plot([r.ep <span class="keyword">for</span> r <span class="keyword">in</span> training_records], [r.reward <span class="keyword">for</span> r <span class="keyword">in</span> training_records])</span><br><span class="line">    plt.title(<span class="string">&#x27;PPO&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Episode&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Moving averaged episode reward&#x27;</span>)</span><br><span class="line">    <span class="comment"># plt.savefig(&quot;img/ppo.png&quot;)</span></span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    plt.plot(lastX, firstY, label=<span class="string">&quot;第一次&quot;</span>)</span><br><span class="line">    plt.plot(lastX, lastY, label=<span class="string">&quot;最后一次&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;cos(&quot;</span>+<span class="string">r&#x27;$\theta$&#x27;</span>+<span class="string">&quot;)&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;time&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line">    <span class="comment"># example = [i for i in range(100)]</span></span><br><span class="line">    <span class="comment"># print(len([i for i in BatchSampler(SubsetRandomSampler(range(100)), 20, False)]))</span></span><br></pre></td></tr></table></figure>
<ul>
<li>效果<img src="/imgs/d0ba034ff53840a354c441cf51e2eacc8194cbbd0fffe8f9ec9d557d97fe54ee.png" alt="picture 4">  </li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-HindsightExperienceReplay（HER）"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/10/13/HindsightExperienceReplay%EF%BC%88HER%EF%BC%89/"
    >HindsightExperienceReplay（HER）</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/13/HindsightExperienceReplay%EF%BC%88HER%EF%BC%89/" class="article-date">
  <time datetime="2022-10-13T06:45:00.000Z" itemprop="datePublished">2022-10-13</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Hindsight-Experience-Replay"><a href="#Hindsight-Experience-Replay" class="headerlink" title="Hindsight Experience Replay"></a>Hindsight Experience Replay</h1><ul>
<li>原文<ul>
<li>Hindsight experience replay</li>
<li>Advances in neural information processing systems</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43145941/article/details/119219436?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-119219436-blog-79498248.pc_relevant_3mothn_strategy_and_data_recovery&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-119219436-blog-79498248.pc_relevant_3mothn_strategy_and_data_recovery&utm_relevant_index=2">参考链接</a><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2></li>
<li>你得知道从状态S到目标goal的映射关系（以机械臂为例，状态可能是多个关节的角度，目标是三维空间一个点的坐标，如果知道状态，那么也能推算出机械臂末端在空间中的坐标）；</li>
<li>你得建立一个新的reward计算机制，它取决于目标goal和状态S，一般当状态S映射的goal’与goal相近时给予奖励；</li>
<li>你得创建一个记录每个episode transition的列表，它的作用是在每个episode结束后进行事后经验回放，具体回放方法之后讲；</li>
<li>RL算法接受的状态维度相较于原始的维度增加了目标goal的维度，也就是RL接受：<ul>
<li><img src="/imgs/dba2e2146bcea81c1cc62e3cafac2015a03a4b67fb2a29916283b8ea130ea278.png" alt="picture 2">  <h2 id="举例的github"><a href="#举例的github" class="headerlink" title="举例的github"></a>举例的github</h2></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ZYunfeii/DRL_algorithm_library/tree/master/DDPG/DDPG_spinningup_HER">HER举例</a></li>
<li>这里episode_cache为存储transition的列表((s1,a1,r1,s1’),(s2,a2,r2,s2’)…)，枚举这个列表的元素。在第二句根据每个transition产生HER_SAMPLE_NUM个新的目标点new_goals，这些<strong>目标点时根据之后的transition的state推算得到的</strong>，当然一种简单的情况就是state。之后对这些new_goals做遍历，对每一个new_goal都重新计算reward，并将transition中s和s’的goal部分替换为new_goal，之后将这个新的transition存储入经验池buffer。这里之所以可以这么做是因为在动作a不变的情况下，改变goal是不会改变从原来的s转移到s’的转移概率的。</li>
<li>先对每个回合中的所有输入做一个reward的评价</li>
<li>然后在整个回合的数据后处理时，循环到i时，从i之后的数据里随机选出一部分作为新的goal，然后利用这些新的goal重新计算i这个数据的reward，然后将其放入到replay buffer中，可能导致replay buffer中包含多条由同一条数据而来但是reward不同的数据条目</li>
<li>可以在每训练一个回合之后更换初始的goal（也就是在筛选之前针对所有对象的goal）达到多目标训练的效果</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ZYunfeii/DRL_algorithm_library/blob/master/DDPG/DDPG_spinningup_HER/main.py">示例代码</a></li>
<li><img src="/imgs/000d30cf3c120ce06399c7b098681c9274e60a801893efe32854f09bf18d13d6.png" alt="picture 5">  </li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-强化学习复习（四）"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/10/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89/"
    >强化学习复习（四）</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89/" class="article-date">
  <time datetime="2022-10-09T07:27:06.000Z" itemprop="datePublished">2022-10-09</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/sweetice/Deep-reinforcement-learning-with-pytorch">基于pytorch实现的强化学习网络</a></p>
<ul>
<li><strong>这个的PPO写的似乎有问题</strong></li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/labmlai/annotated_deep_learning_paper_implementations">强化学习网络集合</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/thu-ml/tianshou">集合 2</a></p>
</li>
<li><h2 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h2></li>
<li><p>PPO是基于Actor-Critic的算法</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/468828804">参考链接</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> Normal</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.sampler <span class="keyword">import</span> BatchSampler, SubsetRandomSampler</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;Solve the Pendulum-v0 with PPO&#x27;</span>)</span><br><span class="line">parser.add_argument(</span><br><span class="line">    <span class="string">&#x27;--gamma&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.9</span>, metavar=<span class="string">&#x27;G&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;discount factor (default: 0.9)&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>, metavar=<span class="string">&#x27;N&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;random seed (default: 0)&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--render&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;render the environment&#x27;</span>)</span><br><span class="line">parser.add_argument(</span><br><span class="line">    <span class="string">&#x27;--log-interval&#x27;</span>,</span><br><span class="line">    <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">    default=<span class="number">10</span>,</span><br><span class="line">    metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">    <span class="built_in">help</span>=<span class="string">&#x27;interval between training status logs (default: 10)&#x27;</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">torch.manual_seed(args.seed)</span><br><span class="line"></span><br><span class="line">TrainingRecord = namedtuple(<span class="string">&#x27;TrainingRecord&#x27;</span>, [<span class="string">&#x27;ep&#x27;</span>, <span class="string">&#x27;reward&#x27;</span>])</span><br><span class="line">Transition = namedtuple(<span class="string">&#x27;Transition&#x27;</span>, [<span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;a_log_p&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s_&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ActorNet, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(<span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line">        self.mu_head = nn.Linear(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">        self.sigma_head = nn.Linear(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.fc(x))</span><br><span class="line">        mu = <span class="number">2.0</span> * F.tanh(self.mu_head(x))</span><br><span class="line">        sigma = F.softplus(self.sigma_head(x))</span><br><span class="line">        <span class="keyword">return</span> (mu, sigma)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CriticNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CriticNet, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(<span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line">        self.v_head = nn.Linear(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.fc(x))</span><br><span class="line">        state_value = self.v_head(x)</span><br><span class="line">        <span class="keyword">return</span> state_value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>():</span></span><br><span class="line"></span><br><span class="line">    clip_param = <span class="number">0.2</span></span><br><span class="line">    max_grad_norm = <span class="number">0.5</span></span><br><span class="line">    ppo_epoch = <span class="number">10</span></span><br><span class="line">    buffer_capacity, batch_size = <span class="number">1000</span>, <span class="number">32</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.training_step = <span class="number">0</span></span><br><span class="line">        self.anet = ActorNet().<span class="built_in">float</span>()</span><br><span class="line">        self.cnet = CriticNet().<span class="built_in">float</span>()</span><br><span class="line">        self.buffer = []</span><br><span class="line">        self.counter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        self.optimizer_a = optim.Adam(self.anet.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line">        self.optimizer_c = optim.Adam(self.cnet.parameters(), lr=<span class="number">3e-4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        state = torch.from_numpy(state).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            (mu, sigma) = self.anet(state)</span><br><span class="line">        dist = Normal(mu, sigma)</span><br><span class="line">        action = dist.sample()</span><br><span class="line">        action_log_prob = dist.log_prob(action)</span><br><span class="line">        action = action.clamp(-<span class="number">2.0</span>, <span class="number">2.0</span>)</span><br><span class="line">        <span class="keyword">return</span> action.item(), action_log_prob.item()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_value</span>(<span class="params">self, state</span>):</span></span><br><span class="line"></span><br><span class="line">        state = torch.from_numpy(state).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            state_value = self.cnet(state)</span><br><span class="line">        <span class="keyword">return</span> state_value.item()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_param</span>(<span class="params">self</span>):</span></span><br><span class="line">        torch.save(self.anet.state_dict(), <span class="string">&#x27;param/ppo_anet_params.pkl&#x27;</span>)</span><br><span class="line">        torch.save(self.cnet.state_dict(), <span class="string">&#x27;param/ppo_cnet_params.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">        self.counter += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.counter % self.buffer_capacity == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.training_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        s = torch.tensor([t.s <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        a = torch.tensor([t.a <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        r = torch.tensor([t.r <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        s_ = torch.tensor([t.s_ <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">        old_action_log_probs = torch.tensor(</span><br><span class="line">            [t.a_log_p <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        r = (r - r.mean()) / (r.std() + <span class="number">1e-5</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            target_v = r + args.gamma * self.cnet(s_)</span><br><span class="line"></span><br><span class="line">        adv = (target_v - self.cnet(s)).detach()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.ppo_epoch):</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> BatchSampler(</span><br><span class="line">                    SubsetRandomSampler(<span class="built_in">range</span>(self.buffer_capacity)), self.batch_size, <span class="literal">False</span>):</span><br><span class="line"></span><br><span class="line">                (mu, sigma) = self.anet(s[index])</span><br><span class="line">                dist = Normal(mu, sigma)</span><br><span class="line">                action_log_probs = dist.log_prob(a[index])</span><br><span class="line">                ratio = torch.exp(action_log_probs - old_action_log_probs[index])</span><br><span class="line"></span><br><span class="line">                surr1 = ratio * adv[index]</span><br><span class="line">                surr2 = torch.clamp(ratio, <span class="number">1.0</span> - self.clip_param,</span><br><span class="line">                                    <span class="number">1.0</span> + self.clip_param) * adv[index]</span><br><span class="line">                action_loss = -torch.<span class="built_in">min</span>(surr1, surr2).mean()</span><br><span class="line"></span><br><span class="line">                self.optimizer_a.zero_grad()</span><br><span class="line">                action_loss.backward()</span><br><span class="line">                nn.utils.clip_grad_norm_(self.anet.parameters(), self.max_grad_norm)</span><br><span class="line">                self.optimizer_a.step()</span><br><span class="line"></span><br><span class="line">                value_loss = F.smooth_l1_loss(self.cnet(s[index]), target_v[index])</span><br><span class="line">                self.optimizer_c.zero_grad()</span><br><span class="line">                value_loss.backward()</span><br><span class="line">                nn.utils.clip_grad_norm_(self.cnet.parameters(), self.max_grad_norm)</span><br><span class="line">                self.optimizer_c.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">del</span> self.buffer[:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    env = gym.make(<span class="string">&#x27;Pendulum-v0&#x27;</span>)</span><br><span class="line">    env.seed(args.seed)</span><br><span class="line"></span><br><span class="line">    agent = Agent()</span><br><span class="line"></span><br><span class="line">    training_records = []</span><br><span class="line">    running_reward = -<span class="number">1000</span></span><br><span class="line">    state = env.reset()</span><br><span class="line">    <span class="keyword">for</span> i_ep <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">        state = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">            action, action_log_prob = agent.select_action(state)</span><br><span class="line">            state_, reward, done, _ = env.step([action])</span><br><span class="line">            <span class="keyword">if</span> args.render:</span><br><span class="line">                env.render()</span><br><span class="line">            <span class="keyword">if</span> agent.store(Transition(state, action, action_log_prob, (reward + <span class="number">8</span>) / <span class="number">8</span>, state_)):</span><br><span class="line">                agent.update()</span><br><span class="line">            score += reward</span><br><span class="line">            state = state_</span><br><span class="line"></span><br><span class="line">        running_reward = running_reward * <span class="number">0.9</span> + score * <span class="number">0.1</span></span><br><span class="line">        training_records.append(TrainingRecord(i_ep, running_reward))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i_ep % args.log_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Ep &#123;&#125;\tMoving average score: &#123;:.2f&#125;\t&#x27;</span>.<span class="built_in">format</span>(i_ep, running_reward))</span><br><span class="line">        <span class="keyword">if</span> running_reward &gt; -<span class="number">200</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Solved! Moving average score is now &#123;&#125;!&quot;</span>.<span class="built_in">format</span>(running_reward))</span><br><span class="line">            env.close()</span><br><span class="line">            agent.save_param()</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;log/ppo_training_records.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                pickle.dump(training_records, f)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    plt.plot([r.ep <span class="keyword">for</span> r <span class="keyword">in</span> training_records], [r.reward <span class="keyword">for</span> r <span class="keyword">in</span> training_records])</span><br><span class="line">    plt.title(<span class="string">&#x27;PPO&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Episode&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Moving averaged episode reward&#x27;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&quot;img/ppo.png&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></li>
<li><p>产生动作的方式与一般的PG类似，就是使用PG输出一个μ和σ然后使用正态分布，然后进行采样输出真正的动作</p>
<h3 id="网络更新"><a href="#网络更新" class="headerlink" title="网络更新"></a>网络更新</h3></li>
<li><p>reward归一化，将reward-平均数/标准差</p>
</li>
<li><p>计算target价值，使用当前回合的reward+系数×Critic对于下一状态的分析</p>
</li>
<li><p>估计价值和实际价值的差：直接用估计当前状态的价值与上一步计算的价值做差</p>
</li>
<li><p>计算actor网络的loss的方法是</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(mu, sigma) = self.anet(s[index])</span><br><span class="line">dist = Normal(mu, sigma)</span><br><span class="line">action_log_probs = dist.log_prob(a[index])</span><br><span class="line">ratio = torch.exp(action_log_probs - old_action_log_probs[index])</span><br><span class="line"></span><br><span class="line">surr1 = ratio * adv[index]</span><br><span class="line">surr2 = torch.clamp(ratio, <span class="number">1.0</span> - self.clip_param,</span><br><span class="line">                    <span class="number">1.0</span> + self.clip_param) * adv[index]</span><br><span class="line">action_loss = -torch.<span class="built_in">min</span>(surr1, surr2).mean()</span><br></pre></td></tr></table></figure></li>
<li><p>先利用当前的网络采样输出μ和σ，然后计算之前action的log_prob，，然后利用表达式<code>e^(这次网络计算出的log可能性-上次网络计算出的log可能性)</code>，实际上就是<code>这次网络计算出的可能性/上次网络计算出的可能性</code>与上一步算出的估计价值和实际价值的差值<strong>修正这个值</strong>，主要目的是用合理的方法<u>利用其他时刻动作的输出，增加一个动作的利用效率</u></p>
</li>
<li><p><img src="/imgs/f8606933037c3541da1fdedf4142d2a36bcb61ee2ddd0dba8b63febd43dfd75c.png" alt="picture 1">  </p>
</li>
<li><p>然后用一个参数×上面的数值，处理之后得到最终需要反向传播的值（actor）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">value_loss = F.smooth_l1_loss(self.cnet(s[index]), target_v[index])</span><br><span class="line">self.optimizer_c.zero_grad()</span><br><span class="line">value_loss.backward()</span><br><span class="line">nn.utils.clip_grad_norm_(self.cnet.parameters(), self.max_grad_norm)</span><br><span class="line">self.optimizer_c.step()</span><br></pre></td></tr></table></figure></li>
<li><p>更新Critic的参数操作是将现在的Critic网络对价值的判断与前面算出的目标价值判断进行比较，将二者的差进行反向传播</p>
<h2 id="BatchSampler和SubsetRamdomSampler"><a href="#BatchSampler和SubsetRamdomSampler" class="headerlink" title="BatchSampler和SubsetRamdomSampler"></a>BatchSampler和SubsetRamdomSampler</h2></li>
<li><p><code>SubsetRandomSampler</code>实际上是将数据的顺序打乱做一个全排列</p>
</li>
<li><p><code>BatchSampler</code>实际上是根据设置的batch_size给数据分成一个个的batch</p>
</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-Anaconda配置问题记录"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/10/09/Anaconda%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"
    >Anaconda配置问题记录</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/09/Anaconda%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/" class="article-date">
  <time datetime="2022-10-09T06:25:18.000Z" itemprop="datePublished">2022-10-09</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="找不到-condarc文件"><a href="#找不到-condarc文件" class="headerlink" title="找不到.condarc文件"></a>找不到<code>.condarc</code>文件</h2><ul>
<li>先执行一次<code>conda config</code>就能找到了<h2 id="配置Anaconda使用clash代理"><a href="#配置Anaconda使用clash代理" class="headerlink" title="配置Anaconda使用clash代理"></a>配置Anaconda使用clash代理</h2></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43135178/article/details/124499250">链接</a><h2 id="修改Anaconda安装环境的默认位置"><a href="#修改Anaconda安装环境的默认位置" class="headerlink" title="修改Anaconda安装环境的默认位置"></a>修改Anaconda安装环境的默认位置</h2></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36455412/article/details/125347552">链接</a><h2 id="手动添加Anaconda到环境变量"><a href="#手动添加Anaconda到环境变量" class="headerlink" title="手动添加Anaconda到环境变量"></a>手动添加Anaconda到环境变量</h2></li>
<li><a target="_blank" rel="noopener" href="http://news.sohu.com/a/446583263_120918998">链接</a><h2 id="配置pytorch使用CPU的多线程"><a href="#配置pytorch使用CPU的多线程" class="headerlink" title="配置pytorch使用CPU的多线程"></a>配置pytorch使用CPU的多线程</h2></li>
<li><code>torch.set_num_threads(8)</code></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/a_piece_of_ppx/article/details/123714865">参考</a></li>
<li>作用不大<h2 id="GYM-官方文档"><a href="#GYM-官方文档" class="headerlink" title="GYM 官方文档"></a>GYM 官方文档</h2></li>
<li><a target="_blank" rel="noopener" href="https://www.gymlibrary.dev/">链接</a><h2 id="word中插入代码"><a href="#word中插入代码" class="headerlink" title="word中插入代码"></a>word中插入代码</h2></li>
<li><a target="_blank" rel="noopener" href="https://highlightcode.com/">https://highlightcode.com/</a></li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/anaconda/" rel="tag">anaconda</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-强化学习复习（三）"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/"
    >强化学习复习（三）</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/" class="article-date">
  <time datetime="2022-10-07T13:12:05.000Z" itemprop="datePublished">2022-10-07</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/pytorch/examples/tree/main/reinforcement_learning">pytorch关于强化学习的示例</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/_modules/torch/distributions/normal.html">pytorch源码</a></p>
<h2 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h2></li>
<li><p>基本上就是通过动作获得的奖励或者惩罚信息反向传播，给<code>Actor</code>网络进行指导</p>
</li>
<li><p><code>Critic</code>实际上是一个类似于<code>QNetwork</code>的网络，它的作用是对Actor的动作做出每个时刻的评价，之前只能在回合结束的时候根据给出的回报进行更新，但是拥有<code>Critic</code>之后就可以在每个时刻进行更新了，也就是**在一个回合结束之前，猜测出这个动作可能导致的reward，并以此指导<code>Actor</code>**。</p>
</li>
<li><p>例子</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> count</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> Categorical</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;PyTorch REINFORCE example&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--gamma&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.99</span>, metavar=<span class="string">&#x27;G&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;discount factor (default: 0.99)&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">543</span>, metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;random seed (default: 543)&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--render&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;render the environment&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--log-interval&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10</span>, metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;interval between training status logs (default: 10)&#x27;</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line">env.seed(args.seed)</span><br><span class="line">torch.manual_seed(args.seed)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Policy</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Policy, self).__init__()</span><br><span class="line">        self.affine1 = nn.Linear(<span class="number">4</span>, <span class="number">128</span>)</span><br><span class="line">        self.dropout = nn.Dropout(p=<span class="number">0.6</span>)</span><br><span class="line">        self.affine2 = nn.Linear(<span class="number">128</span>, <span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">        self.saved_log_probs = []</span><br><span class="line">        self.rewards = []</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.affine1(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        action_scores = self.affine2(x)</span><br><span class="line">        <span class="keyword">return</span> F.softmax(action_scores, dim=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">policy = Policy()</span><br><span class="line">optimizer = optim.Adam(policy.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">eps = np.finfo(np.float64).eps.item()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">state</span>):</span></span><br><span class="line">    state = torch.from_numpy(state).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>)</span><br><span class="line">    probs = policy(state)</span><br><span class="line">    m = Categorical(probs)</span><br><span class="line">    action = m.sample()</span><br><span class="line">    policy.saved_log_probs.append(m.log_prob(action))</span><br><span class="line">    <span class="keyword">return</span> action.item()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">finish_episode</span>():</span></span><br><span class="line">    R = <span class="number">0</span></span><br><span class="line">    policy_loss = []</span><br><span class="line">    returns = []</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> policy.rewards[::-<span class="number">1</span>]:</span><br><span class="line">        R = r + args.gamma * R</span><br><span class="line">        returns.insert(<span class="number">0</span>, R)</span><br><span class="line">    returns = torch.tensor(returns)</span><br><span class="line">    returns = (returns - returns.mean()) / (returns.std() + eps)</span><br><span class="line">    <span class="keyword">for</span> log_prob, R <span class="keyword">in</span> <span class="built_in">zip</span>(policy.saved_log_probs, returns):</span><br><span class="line">        policy_loss.append(-log_prob * R)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    policy_loss = torch.cat(policy_loss).<span class="built_in">sum</span>()</span><br><span class="line">    policy_loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="keyword">del</span> policy.rewards[:]</span><br><span class="line">    <span class="keyword">del</span> policy.saved_log_probs[:]</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    running_reward = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> count(<span class="number">1</span>):</span><br><span class="line">        state, ep_reward = env.reset(), <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10000</span>):  <span class="comment"># Don&#x27;t infinite loop while learning</span></span><br><span class="line">            action = select_action(state)</span><br><span class="line">            state, reward, done, _ = env.step(action)</span><br><span class="line">            <span class="keyword">if</span> args.render:</span><br><span class="line">                env.render()</span><br><span class="line">            policy.rewards.append(reward)</span><br><span class="line">            ep_reward += reward</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"> </span><br><span class="line">        running_reward = <span class="number">0.05</span> * ep_reward + (<span class="number">1</span> - <span class="number">0.05</span>) * running_reward</span><br><span class="line">        finish_episode()</span><br><span class="line">        <span class="keyword">if</span> i_episode % args.log_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Episode &#123;&#125;\tLast reward: &#123;:.2f&#125;\tAverage reward: &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                  i_episode, ep_reward, running_reward))</span><br><span class="line">        <span class="keyword">if</span> running_reward &gt; env.spec.reward_threshold:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Solved! Running reward is now &#123;&#125; and &quot;</span></span><br><span class="line">                  <span class="string">&quot;the last episode runs to &#123;&#125; time steps!&quot;</span>.<span class="built_in">format</span>(running_reward, t))</span><br><span class="line">            torch.save(policy.state_dict(),<span class="string">&#x27;hello.pt&#x27;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></li>
<li><p><code>Categorical</code>：<img src="/imgs/bbff369c31599485cece0ecd1f86a3c2d5f7d254eeb1b8f76de5053a207275b2.png" alt="picture 49">  </p>
<ul>
<li><code>log_probs</code><img src="/imgs/b02e3510312ce797463f7218ef7fde86af78b655a54491b1bee2766aac44d92d.png" alt="picture 50">  ，实际上就是将对应动作发生的可能性求了log<h3 id="给出策略的具体操作"><a href="#给出策略的具体操作" class="headerlink" title="给出策略的具体操作"></a>给出策略的具体操作</h3></li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/439220435">参考链接</a></p>
</li>
<li><p>给出策略的具体操作是先将输入经过一系列网络的运算之后，经过softmax归一化，得到和为1的几个输出。然后在输出过程中对于具有这几种概率的输出进行随机取样，得到最终的输出动作。（<strong>离散动作</strong>）</p>
</li>
<li><p>针对<strong>连续动作</strong>，可以将整个网络的输出更改为输出一个高斯分布函数的μ值（均值），结合用户指定的σ（方差），即可形成一个高斯分布，然后通过类似的sample采样即可得出需要的动作。注意训练阶段为了实现有效的exploration，不要使用太小的σ，否则因为输出太集中没法找到实际上的最优解。</p>
<ul>
<li>也可以让网络也输出σ</li>
<li>反向传播的思路相似，也是直接利用<code>torch.Distributions.Normal</code>的<code>log_prob</code>函数输出概率的log值<img src="/imgs/da881cab2fe598db70c03731a3ec37aec64f13d6f84d04168d697c1aa1d390d6.png" alt="picture 51"></li>
</ul>
</li>
</ul>
<h3 id="网络更新"><a href="#网络更新" class="headerlink" title="网络更新"></a>网络更新</h3><ul>
<li><img src="/imgs/b734753c9daa77b5104e2738198288151b8eafd8ec96a44556debd73f729b630.png" alt="picture 48">  <h2 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h2></li>
<li>现在我们来说说 DDPG 中所用到的神经网络. 它其实和我们之前提到的 Actor-Critic 形式差不多, 也需要有基于 策略 Policy 的神经网络 和基于 价值 Value 的神经网络, 但是为了体现 DQN 的思想, 每种神经网络我们都需要再细分为两个, Policy Gradient 这边, 我们有估计网络和现实网络, 估计网络用来输出实时的动作, 供 actor 在现实中实行. 而现实网络则是用来更新价值网络系统的. 所以我们再来看看价值系统这边, 我们也有现实网络和估计网络, 他们都在输出这个状态的价值, 而输入端却有不同, <strong>状态现实网络这边会拿着从动作现实网络来的动作加上状态的观测值加以分析, 而状态估计网络则是拿着当时 Actor 施加的动作当做输入</strong>.在实际运用中, DDPG 的这种做法的确带来了更有效的学习过程.</li>
<li><img src="/imgs/6a83e078a6bdb97f3a4cd97e1da8d01ea4068866f7bf6254e44e27d51c3f03ea.png" alt="picture 43">  <h3 id="学习的过程"><a href="#学习的过程" class="headerlink" title="学习的过程"></a>学习的过程</h3><h4 id="Critic网络"><a href="#Critic网络" class="headerlink" title="Critic网络"></a>Critic网络</h4></li>
<li><code>y_true</code>是要学习的值，这个值是通过<code>Critci</code>的<code>target</code>网络对于下一时刻的<code>actor</code>的<code>target</code>网络的动作做出的评估加上这一时刻的汇报<code>reward</code>计算出来的，而它自身需要修改的值就是直接对当前的环境观测和动作做出的值的判断<code>y_pred</code><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">critic_learn</span>():</span></span><br><span class="line">    a1 = self.actor_target(s1).detach()</span><br><span class="line">    y_true = r1 + self.gamma * self.critic_target(s1, a1).detach()</span><br><span class="line">    </span><br><span class="line">    y_pred = self.critic(s0, a0)</span><br><span class="line">    </span><br><span class="line">    loss_fn = nn.MSELoss()</span><br><span class="line">    loss = loss_fn(y_pred, y_true)</span><br><span class="line">    self.critic_optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    self.critic_optim.step()</span><br></pre></td></tr></table></figure></li>
<li>然后按照一定的比例<code>soft_update</code>对应的<code>target</code>网络即可<h4 id="Actor网络"><a href="#Actor网络" class="headerlink" title="Actor网络"></a>Actor网络</h4></li>
<li>直接利用<code>critic</code>网络对于此刻环境的观测和在此刻环境下<code>actor</code>网络的行为做出评价，然后直接反向传播</li>
<li>同样<code>soft_update</code>另一个<code>target</code>网络即可<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">actor_learn</span>():</span></span><br><span class="line">    loss = -torch.mean( self.critic(s0, self.actor(s0)) )</span><br><span class="line">    self.actor_optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    self.actor_optim.step()</span><br></pre></td></tr></table></figure>
<h2 id="A3C"><a href="#A3C" class="headerlink" title="A3C"></a>A3C</h2></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/pytorch-A3C">pytorch A3C参考</a></li>
<li><img src="/imgs/8bed5783887792cdbaffbae4046098a68b2539e3a7093d5a4624fb6bb1378a00.png" alt="picture 44">  </li>
<li><img src="/imgs/bf942436af1a670b0ab91f0705dd4a8136963ade316c598d29a6a775f107841f.png" alt="picture 45">  </li>
<li><img src="/imgs/daf4381936a5297b7e62f97c553d7c997f51b7ebbecfaee2b4b204c1dd156fba.png" alt="picture 46">  </li>
<li><img src="/imgs/1cdcd316476945548933f8ad5922351079bb5ee5808860489a251970aa3c805d.png" alt="picture 47">  </li>
<li>实际上每个本地网络都是一个Actor-Critic的网络，损失分为动作网络<code>Actor</code>的loss和<code>Critic</code>网络的loss<ul>
<li>Critic的loss可以先计算<code>td_error</code>，用Critic在此时的环境中计算出的值与实际上每一步得到的增加随时间衰减的因子之后的实际上的Reward做差，然后平方即可得到Critic的loss</li>
<li>Actor的loss则是使用反向求出刚才动作的log_prob（怎么求上文有），然后再求出<code>entropy</code>，公式为<img src="/imgs/082feca831bedf90bf6c3253215c0e01b8c26b2ba406997eb4acb361db659f73.png" alt="`0.5 + 0.5 * math.log(2 * math.pi) + torch.log(m.scale)` 1">，然后log_prob×上文的td_error+一个系数×entropy，然后整个计算出来之后取相反数即可得到Actor的loss，然后将整个的两个loss取平均数，反向传播更新参数即可<u>（因为根据计算图倒推可以分别得到组成这个变量的两个变量分别的影响因素，所以不影响反向传播分别更新两个网络）</u>。<h3 id="torch中backword是怎么用的"><a href="#torch中backword是怎么用的" class="headerlink" title="torch中backword是怎么用的"></a>torch中backword是怎么用的</h3></li>
</ul>
</li>
<li>针对<strong>标量</strong>做出的对计算图的反向传播，得到标量的值，算出计算图中每个变量对于得到这个标量的偏导数<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/168748668">参考</a></li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/6/">上一页</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/8/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2021-2024
        <i class="ri-heart-fill heart_icon"></i> FrankZhang
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/avatar.png" alt="Frank’s blogs"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>