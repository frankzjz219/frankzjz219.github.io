<!DOCTYPE html>


<html lang="Chinese">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> Frank’s blogs</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/FLogo.png" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/bkgPic.png" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">Frank’s blogs</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.staticfile.org/typed.js/2.0.12/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['Linux开发和工科生折腾集锦', '算法题总结', 'C/C++/python等开发'],
        startDelay: 0,
        typeSpeed: 100,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  
  
<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content">Linux编程、C/C++开发、算法题、各种工科生折腾开发集锦</div>
</div>


<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-VMWare安装安卓虚拟机"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/01/19/VMWare%E5%AE%89%E8%A3%85%E5%AE%89%E5%8D%93%E8%99%9A%E6%8B%9F%E6%9C%BA/"
    >VMWare安装安卓虚拟机</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/01/19/VMWare%E5%AE%89%E8%A3%85%E5%AE%89%E5%8D%93%E8%99%9A%E6%8B%9F%E6%9C%BA/" class="article-date">
  <time datetime="2023-01-19T02:02:30.000Z" itemprop="datePublished">2023-01-19</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="VMWare安装安卓虚拟机"><a href="#VMWare安装安卓虚拟机" class="headerlink" title="VMWare安装安卓虚拟机"></a>VMWare安装安卓虚拟机</h1><ul>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41930631/article/details/125922411">参考链接</a></li>
<li><a target="_blank" rel="noopener" href="https://osdn.net/projects/android-x86/releases/66512">Android-x86下载地址</a></li>
<li><a target="_blank" rel="noopener" href="https://www.android-x86.org/">Android x86网站</a></li>
<li>下载的文件：<strong>cm-x86-14.1-r5.iso</strong> 对应的版本：<img src="/imgs/9c996e27832f31e785c3ff0f754b076708d8c3a03bd70ffd5a84aac637f54ab3.png" alt="picture 11">  </li>
<li>下载的文件:** android-x86_64-9.0-r2-k49.iso**对应的版本<img src="/imgs/3b670a9ec837461bbf26ce397d220bbf0338408339c34afa45bbdffeb1ea8d74.png" alt="picture 12">  </li>
</ul>
<h2 id="部分步骤截图"><a href="#部分步骤截图" class="headerlink" title="部分步骤截图"></a>部分步骤截图</h2><ul>
<li>可能无法检测操作系统，此时只需要手动选择FreeBSD如图即可<ul>
<li><img src="/imgs/dc56bcd5c5212c990e728b364b4501628587d7633b07f18a735cbd3904ca18c7.png" alt="picture 1">  </li>
<li><img src="/imgs/820c7f377a64d81192f6d0f5d95bfd021fcfe23c53be10bf59e53e0533e9a95a.png" alt="picture 2">  </li>
</ul>
</li>
<li>下一步磁盘记得勾选这个<ul>
<li><img src="/imgs/dacecac03215b7a999b022bc6394b7590497df64d979e756401fded87a56d749.png" alt="picture 3">  </li>
</ul>
</li>
<li>此时记得打开设置关闭CD驱动器等<ul>
<li><img src="/imgs/810585dce08781495adb429f9dd926f58bbfec8f4359a18f3c253a3c16fd4b35.png" alt="picture 4">  </li>
</ul>
</li>
<li>然后正常重启即可</li>
<li>有时开机的时候回卡在黑屏界面，屏幕上角有光标但是卡住不动</li>
<li>此时重启到debug模式<ul>
<li><img src="/imgs/fc7bd905a97f9a932f1e545c4563f47540638ea4464aaa8520504f0da36b1452.png" alt="picture 5">  </li>
<li>在命令行输入<code>vi /mnt/grub/menu.lst</code></li>
<li>红线位置添加<img src="/imgs/64d3b05b4778357e2e701b7390061551a6a73383ce93ea2f4ef578eb843edf64.png" alt="picture 6">  </li>
<li>添加完成之后再次利用VMWare手动重启即可打开图形界面<ul>
<li><img src="/imgs/3841d33e89117672f772a6b0967f918a34e131b7c92b5f18ae9bf98d923cd436.png" alt="picture 7">  </li>
</ul>
</li>
<li>编辑虚拟网络<ul>
<li><img src="/imgs/da29b0f7a771e985201634fe3f05c8bc951f84f263a8ce49546beb0fa60b5983.png" alt="picture 8">  </li>
</ul>
</li>
<li>更改虚拟机网络使得虚拟机可以访问互联网<ul>
<li><img src="/imgs/5e5ab8ac742eec624c9639a85c96b6074b3907a6bd5266e170909180d38f49c4.png" alt="picture 9">  </li>
</ul>
</li>
<li>此时会出现<img src="/imgs/efe0e0d718b98bc6debf0144694743d6f5cb27ed4ef4bacb1aca891f69bd5e45.png" alt="picture 10">  </li>
</ul>
</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/" rel="tag">虚拟机</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-python随记"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/12/24/python%E9%9A%8F%E8%AE%B0/"
    >python随记</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/12/24/python%E9%9A%8F%E8%AE%B0/" class="article-date">
  <time datetime="2022-12-24T05:53:12.000Z" itemprop="datePublished">2022-12-24</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="matplotlib"><a href="#matplotlib" class="headerlink" title="matplotlib"></a>matplotlib</h2><ul>
<li>基本框架<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 指定默认字体</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 解决保存图像时符号-显示为方块的2问题</span></span><br><span class="line">plt.plot(xs, yx, label=<span class="string">&quot;x随迭代次数的变化&quot;</span>)</span><br><span class="line">plt.plot(xs, yy, label=<span class="string">&quot;y随迭代次数的变化&quot;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">&quot;迭代次数&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;x以及f(x)的数值&quot;</span>)</span><br><span class="line">plt.title(<span class="string">r&quot;$x_0$ = 1&quot;</span>)</span><br><span class="line">plt.show()  </span><br></pre></td></tr></table></figure></li>
<li>使用latex语法：<code>r&quot;$&lt;latex语法&gt;$&quot;</code><h2 id="pycharm配置项目的python解释器"><a href="#pycharm配置项目的python解释器" class="headerlink" title="pycharm配置项目的python解释器"></a>pycharm配置项目的python解释器</h2></li>
<li>设置下面找到项目然后更改解释器<ul>
<li><img src="/imgs/73243950c62d71fd56e29802053af5f4ce772fb1ac73bfc9098e862fdd88ee36.png" alt="picture 1">  </li>
</ul>
</li>
<li><strong>注意单独更改此处没有用</strong><ul>
<li><img src="/imgs/ae6810ecdb55ecf83bab089970463402519d5ed979d75e1d7005bed717d6c11d.png" alt="picture 2">  </li>
</ul>
</li>
<li>python得到当前路径<ul>
<li><code>os.getcwd().replace(&#39;\\&#39;,&#39;/&#39;)</code>此处将<code>/</code>替换为了<code>\\</code>，实际上是单斜线<h3 id="数组切片"><a href="#数组切片" class="headerlink" title="数组切片"></a>数组切片</h3></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42782150/article/details/127014616">参考链接</a></li>
<li>注意<code>数组[begin:end]</code>切到的是begin到<strong>end-1</strong>的内容<h3 id="重启程序"><a href="#重启程序" class="headerlink" title="重启程序"></a>重启程序</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 获取当前解释器路径</span></span><br><span class="line">p = sys.executable</span><br><span class="line"><span class="comment"># 启动新程序(解释器路径, 当前程序)</span></span><br><span class="line">os.execl(p, p, *sys.argv)</span><br><span class="line"><span class="comment"># 关闭当前程序</span></span><br><span class="line">sys.exit()</span><br></pre></td></tr></table></figure>
<h3 id="pycharm内存不足"><a href="#pycharm内存不足" class="headerlink" title="pycharm内存不足"></a>pycharm内存不足</h3></li>
<li><a target="_blank" rel="noopener" href="https://www.ycpai.cn/python/dwqRcLg8.html">教程</a></li>
<li><img src="/imgs/66393c1b6f785f70b6cfa70b8b83a6f8705de4000772b5425184113d7106cb15.png" alt="picture 3">  </li>
</ul>
<h3 id="import-numpy的时候出错"><a href="#import-numpy的时候出错" class="headerlink" title="import numpy的时候出错"></a>import numpy的时候出错</h3><ul>
<li>有时候在一些<strong>非x86</strong>的设备上使用numpy会报错 <code>Illegal instruction (core dumped)</code></li>
<li><img src="/imgs/e8f026142d2e9e89d431c908409bd83cd625957400511b4df2b0ee781c22d5da.png" alt="picture 3">  </li>
<li>此时需要设置环境变量<code>~/.bashrc</code>追加一句<code>export OPENBLAS_CORETYPE=ARMV8</code>然后<code>source ~/.bashrc</code></li>
<li>此时不再报错<ul>
<li><img src="/imgs/78f122d9e620fc70c253acd0eb657b711ecd8026371a6ef3c360dd744739cc03.png" alt="picture 4">  </li>
</ul>
</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/matplotlib/" rel="tag">matplotlib</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/" rel="tag">python</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-Unity学习笔记"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/12/21/Unity%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"
    >Unity学习笔记</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/12/21/Unity%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="article-date">
  <time datetime="2022-12-20T16:21:08.000Z" itemprop="datePublished">2022-12-21</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Unity学习笔记"><a href="#Unity学习笔记" class="headerlink" title="Unity学习笔记"></a>Unity学习笔记</h1><p><u><strong>先安装hub再安装Unity！！！</strong></u></p>
<h2 id="环境变量配置（建议用VS一步到位，VSCode配置环境太麻烦）"><a href="#环境变量配置（建议用VS一步到位，VSCode配置环境太麻烦）" class="headerlink" title="环境变量配置（建议用VS一步到位，VSCode配置环境太麻烦）"></a>环境变量配置（建议用VS一步到位，VSCode配置环境太麻烦）</h2><ul>
<li>右键文件管理器-&gt;属性-&gt;<img src="/imgs/7a45dd2b4ae080fe76c4387e1d39b34498f2d3b18611f42a1274bc17a4d2e25c.png" alt="picture 5">  </li>
<li>然后环境变量，添加<img src="/imgs/047fbdff8062f0b8d0fe2806b51cccd884cadc4c8be1c8fb6cddd968ea217925.png" alt="picture 6">  看这俩有没有，然后试一下命令行输入<code>dotnet --info</code></li>
</ul>
<h2 id="教程"><a href="#教程" class="headerlink" title="教程"></a>教程</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV16W411D7sC?p=14&vd_source=2ad101445b39a9dac0437fdcd408895e">入门教程</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://docs.unity3d.com/2022.1/Documentation/ScriptReference/">官方文档</a></p>
<h2 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h2></li>
<li><p>用将对象拖拽到这个位置<img src="/imgs/fd77db333d98e6cb810a31c94a1629c8359b6da349617469610875b303c9798a.png" alt="picture 4"><br>的方式给<code>public</code>对象赋值，包括<code>GameObject</code>和<code>RigidBody</code>，这两个得<u>分开赋值</u></p>
</li>
<li><p><code>RigidBody</code>的<code>AddForce</code>不能开启<img src="/imgs/fdedeaddbfac4dea740aae258bcd25e49c578e1b4a6cc61b9238483bb50d35d8.png" alt="picture 2">  </p>
</li>
<li><p>刚体的<a target="_blank" rel="noopener" href="https://blog.csdn.net/LOVE_Me__/article/details/125851048">教程</a></p>
</li>
<li><p>is Kinematic的<a target="_blank" rel="noopener" href="https://blog.csdn.net/iiiiiiimp/article/details/126822387">教程</a></p>
<ul>
<li>isKinematic<strong>不会对碰撞和力做出反应</strong>，不受物理系统影响，但依然<strong>会对其他刚体产生物理影响</strong>（比如可以<strong>阻挡</strong>其他刚体）。</li>
<li>isKinematic只能<u>在脚本中修改物体的Transform属性来移动</u>。</li>
<li>用在经常需要移动等变化物理状态的碰撞体上。一个刚体碰撞体，可以随时开启或关闭Is Kinematic选项，不会像静态碰撞体的enabled开启或关闭那样引起物理系统的问题。</li>
</ul>
</li>
<li><p>给一个物理系统的刚体添加一个瞬时的速度的方法</p>
<ul>
<li><code>wbRd.AddForce(new Vector3(x, y, z) * 1.0f, ForceMode.Impulse);</code></li>
<li>或者ForceMode设置为<code>VelocityChange</code>，可以直接改变速度，类似于碰撞的效果</li>
</ul>
</li>
<li><p>获取时间用<code>Time</code>类，unity有支持</p>
</li>
<li><p>复位一个场景用<code>SceneManager.LoadScene(index);</code>，index是这个scene在最终的序列里拍第几个，从0开始</p>
</li>
<li><p>一个<code>Vector3.normalized</code>给出同方向的一个单位向量</p>
</li>
<li><p><code>transform.LookAt(transform)</code>是让当前对象的z轴指向目标对象（z轴是相机的拍照方向）</p>
</li>
<li><p>不规则物体生成碰撞体：<img src="/imgs/d2d0ca79325ddcc42b7949239c75d296bb0b0b5bdd12eb91330d0a4f2f44c5e6.png" alt="picture 3">  </p>
</li>
<li><p>数学计算用<code>Mathf</code>对象下面的操作函数，其中的三角函数是角度制（0-360°）的</p>
</li>
<li><p>鼠标位置用<code>Input.mousePosition</code>得到一个Vector2</p>
</li>
<li><p>键盘用<code>Input.GetKeyDown(KeyCode.按键名)</code>或者其他，可以查手册，KeyCode包含的内容也查手册</p>
</li>
<li><p><code>Transform.translate()</code>函数可以指定运动的坐标系是自身的还是世界的</p>
</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Unity/" rel="tag">Unity</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-机器学习知识帖"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/10/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E5%B8%96/"
    >机器学习知识帖</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E5%B8%96/" class="article-date">
  <time datetime="2022-10-31T07:45:40.000Z" itemprop="datePublished">2022-10-31</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33992985">梯度下降法</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34842727">正规方程法(最小二乘法)</a></p>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2></li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28408516">逻辑回归</a></p>
<h2 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h2></li>
<li><p>在逻辑回归中，最常用的是代价函数是交叉熵(Cross Entropy)，交叉熵是一个常见的代价函数，在神经网络中也会用到。下面是《神经网络与深度学习》一书对交叉熵的解释：</p>
</li>
<li><p>交叉熵是对「出乎意料」（译者注：原文使用suprise）的度量。神经元的目标是去计算函数x→y=y(x)。但是我们让它取而代之计算函数x→a=a(x)。假设我们把a当作y等于1的概率，1−a是y等于0的概率。那么，交叉熵衡量的是我们在知道y的真实值时的平均「出乎意料」程度。当输出是我们期望的值，我们的「出乎意料」程度比较低；当输出不是我们期望的，我们的「出乎意料」程度就比较高。</p>
</li>
<li><p>在1948年，克劳德·艾尔伍德·香农将热力学的熵，引入到信息论，因此它又被称为香农熵(Shannon Entropy)，它是香农信息量(Shannon Information Content, SIC)的期望。香农信息量用来度量不确定性的大小：一个事件的香农信息量等于0，表示该事件的发生不会给我们提供任何新的信息，例如确定性的事件，发生的概率是1，发生了也不会引起任何惊讶；当不可能事件发生时，香农信息量为无穷大，这表示给我们提供了无穷多的新信息，并且使我们无限的惊讶</p>
</li>
<li><p>[交叉熵](<a target="_blank" rel="noopener" href="https://blog.csdn.net/rtygbwwwerr/article/details/50778098">https://blog.csdn.net/rtygbwwwerr/article/details/50778098</a></p>
</li>
<li><p><img src="/imgs/320549d4337620d61dd7b081632aa72e223e40cb5484ebaedd5756de3236de03.png" alt="picture 1">  </p>
<ul>
<li>上述式子是熵的期望（不同情况的熵的加权和）</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28415991">代价函数</a></p>
<h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2></li>
<li><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/v_july_v/article/details/7624837">SVM原理</a></p>
<h2 id="KMeans"><a href="#KMeans" class="headerlink" title="KMeans"></a>KMeans</h2></li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/78798251?utm_source=qq">KMeans</a></p>
<h2 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h2></li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/110807201">Dueling DQN</a></p>
<h2 id="文章链接"><a href="#文章链接" class="headerlink" title="文章链接"></a>文章链接</h2></li>
<li><p><a target="_blank" rel="noopener" href="https://openai.com/blog/vpt/">openai用强化学习玩我的世界</a></p>
</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-贝叶斯回归"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/10/16/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9B%9E%E5%BD%92/"
    >贝叶斯回归</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/16/%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%9B%9E%E5%BD%92/" class="article-date">
  <time datetime="2022-10-16T03:20:52.000Z" itemprop="datePublished">2022-10-16</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/436569049">参考链接</a><br><a target="_blank" rel="noopener" href="https://www.zhihu.com/people/tao-tie-wen-10/posts">总链接</a></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-PPO调参实录"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/10/13/PPO%E8%B0%83%E5%8F%82%E5%AE%9E%E5%BD%95/"
    >PPO调参实录</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/13/PPO%E8%B0%83%E5%8F%82%E5%AE%9E%E5%BD%95/" class="article-date">
  <time datetime="2022-10-13T10:54:17.000Z" itemprop="datePublished">2022-10-13</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="PPO调参实录"><a href="#PPO调参实录" class="headerlink" title="PPO调参实录"></a>PPO调参实录</h1><h2 id="采用甄别reward的replay-buffer"><a href="#采用甄别reward的replay-buffer" class="headerlink" title="采用甄别reward的replay buffer"></a>采用甄别reward的replay buffer</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">store</span>(<span class="params">self, transition</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(self.buffer)&gt;=self.buffer_capacity:</span><br><span class="line">        tmp = np.random.randint(low=<span class="number">0</span>, high=<span class="built_in">len</span>(self.buffer))</span><br><span class="line">        <span class="keyword">if</span> self.buffer[tmp].r &gt; transition.r:</span><br><span class="line">            <span class="keyword">if</span> np.random.randint(low=<span class="number">0</span>, high=<span class="number">2</span>) == <span class="number">1</span>:  <span class="comment"># 一半的概率 替换的时候会保留reward更高的</span></span><br><span class="line">                self.buffer[tmp] = transition</span><br><span class="line">        self.buffer[tmp] = transition</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">    self.counter += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> self.counter % self.buffer_capacity == <span class="number">0</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>结果</p>
<ul>
<li><img src="/imgs/e94f577e49117f5b41cc7faa4f930abd7e60400dbb265f1cc2530435a3534e2f.png" alt="image.png 1">  </li>
</ul>
</li>
<li><p>甄别的比率降到1/3之后</p>
<ul>
<li><img src="/imgs/71d730a255173dfdc717d546f3cb9815b2b53c00e33de147e389c889754c1d03.png" alt="picture 4">  </li>
</ul>
</li>
</ul>
<h2 id="优化（22年11月13日）的网络"><a href="#优化（22年11月13日）的网络" class="headerlink" title="优化（22年11月13日）的网络"></a>优化（22年11月13日）的网络</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> Normal</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.sampler <span class="keyword">import</span> BatchSampler, SubsetRandomSampler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">torch.set_num_threads(<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;Solve the Pendulum-v0 with PPO&#x27;</span>)</span><br><span class="line">parser.add_argument(</span><br><span class="line">    <span class="string">&#x27;--gamma&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.9</span>, metavar=<span class="string">&#x27;G&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;discount factor (default: 0.9)&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>, metavar=<span class="string">&#x27;N&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;random seed (default: 0)&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--render&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;render the environment&#x27;</span>)</span><br><span class="line">parser.add_argument(</span><br><span class="line">    <span class="string">&#x27;--log-interval&#x27;</span>,</span><br><span class="line">    <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">    default=<span class="number">10</span>,</span><br><span class="line">    metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">    <span class="built_in">help</span>=<span class="string">&#x27;interval between training status logs (default: 10)&#x27;</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">torch.manual_seed(args.seed)</span><br><span class="line"></span><br><span class="line">TrainingRecord = namedtuple(<span class="string">&#x27;TrainingRecord&#x27;</span>, [<span class="string">&#x27;ep&#x27;</span>, <span class="string">&#x27;reward&#x27;</span>])</span><br><span class="line">Transition = namedtuple(<span class="string">&#x27;Transition&#x27;</span>, [<span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;a_log_p&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s_&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ActorNet, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(<span class="number">3</span>, <span class="number">200</span>)</span><br><span class="line">        self.mu_head = nn.Linear(<span class="number">200</span>, <span class="number">1</span>)</span><br><span class="line">        self.sigma_head = nn.Linear(<span class="number">200</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.fc(x))</span><br><span class="line">        mu = <span class="number">2.0</span> * torch.tanh(self.mu_head(x))</span><br><span class="line">        sigma = F.softplus(self.sigma_head(x))</span><br><span class="line">        <span class="keyword">return</span> (mu, sigma)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CriticNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CriticNet, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(<span class="number">3</span>, <span class="number">200</span>)</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">200</span>, <span class="number">200</span>)</span><br><span class="line">        self.v_head = nn.Linear(<span class="number">200</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.fc(x))</span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        state_value = self.v_head(x)</span><br><span class="line">        <span class="keyword">return</span> state_value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>():</span></span><br><span class="line">    clip_param = <span class="number">0.1</span></span><br><span class="line">    max_grad_norm = <span class="number">0.5</span></span><br><span class="line">    ppo_epoch = <span class="number">5</span></span><br><span class="line">    buffer_capacity, batch_size = <span class="number">1000</span>, <span class="number">32</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.training_step = <span class="number">0</span></span><br><span class="line">        self.anet = ActorNet().<span class="built_in">float</span>()</span><br><span class="line">        self.cnet = CriticNet().<span class="built_in">float</span>()</span><br><span class="line">        self.buffer = []</span><br><span class="line">        self.counter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        self.optimizer_a = optim.Adam(self.anet.parameters(), lr=<span class="number">4e-5</span>)</span><br><span class="line">        self.optimizer_c = optim.Adam(self.cnet.parameters(), lr=<span class="number">6e-5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        state = torch.from_numpy(state).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            (mu, sigma) = self.anet(state)</span><br><span class="line">        dist = Normal(mu, sigma)</span><br><span class="line">        action = dist.sample()</span><br><span class="line">        action_log_prob = dist.log_prob(action)</span><br><span class="line">        action = action.clamp(-<span class="number">2.0</span>, <span class="number">2.0</span>)</span><br><span class="line">        <span class="keyword">return</span> action.item(), action_log_prob.item()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_value</span>(<span class="params">self, state</span>):</span></span><br><span class="line"></span><br><span class="line">        state = torch.from_numpy(state).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            state_value = self.cnet(state)</span><br><span class="line">        <span class="keyword">return</span> state_value.item()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_param</span>(<span class="params">self</span>):</span></span><br><span class="line">        torch.save(self.anet.state_dict(), <span class="string">&#x27;param/ppo_anet_params.pkl&#x27;</span>)</span><br><span class="line">        torch.save(self.cnet.state_dict(), <span class="string">&#x27;param/ppo_cnet_params.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store</span>(<span class="params">self, transition</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.buffer)&gt;=self.buffer_capacity:</span><br><span class="line">            tmp = np.random.randint(low=<span class="number">0</span>, high=<span class="built_in">len</span>(self.buffer))</span><br><span class="line">            <span class="keyword">if</span> self.buffer[tmp].r &gt; transition.r:</span><br><span class="line">                <span class="keyword">if</span> np.random.randint(low=<span class="number">0</span>, high=<span class="number">3</span>) == <span class="number">1</span>:  <span class="comment"># 一半的概率 替换的时候会保留reward更高的</span></span><br><span class="line">                    self.buffer[tmp] = transition</span><br><span class="line">            self.buffer[tmp] = transition</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.buffer.append(transition)</span><br><span class="line">        self.counter += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.counter % self.buffer_capacity == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.training_step += <span class="number">1</span></span><br><span class="line">        self.counter = <span class="number">1</span></span><br><span class="line">        s = torch.tensor([t.s <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        a = torch.tensor([t.a <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        r = torch.tensor([t.r <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        s_ = torch.tensor([t.s_ <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">        old_action_log_probs = torch.tensor(</span><br><span class="line">            [t.a_log_p <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        r = (r - r.mean()) / (r.std() + <span class="number">1e-5</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            target_v = r + args.gamma * self.cnet(s_)</span><br><span class="line"></span><br><span class="line">        adv = (target_v - self.cnet(s)).detach()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.ppo_epoch):</span><br><span class="line">            <span class="comment"># indexArray = []</span></span><br><span class="line">            <span class="comment"># dist = Normal(self.buffer_capacity, self.buffer_capacity / 4)</span></span><br><span class="line">            <span class="comment"># sampleList = []</span></span><br><span class="line">            <span class="comment"># cntArray = [0 for i in range(self.buffer_capacity)]</span></span><br><span class="line">            <span class="comment"># # print(cntArray)</span></span><br><span class="line">            <span class="comment"># dist = Normal(self.batch_size*2/3, self.batch_size)</span></span><br><span class="line">            <span class="comment"># for i in range(self.batch_size):</span></span><br><span class="line">            <span class="comment">#     cnt = 0</span></span><br><span class="line">            <span class="comment">#     samples = []</span></span><br><span class="line">            <span class="comment">#     while cnt &lt; self.batch_size:</span></span><br><span class="line">            <span class="comment">#         # print(dist.sample())</span></span><br><span class="line">            <span class="comment">#         # print(cnt)</span></span><br><span class="line">            <span class="comment">#         tmp = round(float(dist.sample()))</span></span><br><span class="line">            <span class="comment">#         if 0 &lt;= tmp and tmp &lt; self.buffer_capacity:</span></span><br><span class="line">            <span class="comment">#             if not tmp in samples:</span></span><br><span class="line">            <span class="comment">#                 if not cntArray[tmp]&gt;4:</span></span><br><span class="line">            <span class="comment">#                     cnt += 1</span></span><br><span class="line">            <span class="comment">#                     samples.append(tmp)</span></span><br><span class="line">            <span class="comment">#                     cntArray[tmp]+=1</span></span><br><span class="line">            <span class="comment">#     sampleList.append(samples)</span></span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> BatchSampler(</span><br><span class="line">                    SubsetRandomSampler(<span class="built_in">range</span>(self.buffer_capacity)), self.batch_size, <span class="literal">False</span>):</span><br><span class="line">            <span class="comment"># for index in sampleList:</span></span><br><span class="line">                <span class="comment"># print(len(index))</span></span><br><span class="line">                (mu, sigma) = self.anet(s[index])</span><br><span class="line">                <span class="comment"># print(mu)</span></span><br><span class="line">                dist = Normal(mu, sigma)</span><br><span class="line">                action_log_probs = dist.log_prob(a[index])</span><br><span class="line">                ratio = torch.exp(action_log_probs - old_action_log_probs[index])</span><br><span class="line"></span><br><span class="line">                surr1 = ratio * adv[index]</span><br><span class="line">                surr2 = torch.clamp(ratio, <span class="number">1.0</span> - self.clip_param,</span><br><span class="line">                                    <span class="number">1.0</span> + self.clip_param) * adv[index]</span><br><span class="line">                action_loss = -torch.<span class="built_in">min</span>(surr1, surr2).mean()</span><br><span class="line"></span><br><span class="line">                self.optimizer_a.zero_grad()</span><br><span class="line">                action_loss.backward()</span><br><span class="line">                nn.utils.clip_grad_norm_(self.anet.parameters(), self.max_grad_norm)</span><br><span class="line">                self.optimizer_a.step()</span><br><span class="line"></span><br><span class="line">                value_loss = F.smooth_l1_loss(self.cnet(s[index]), target_v[index])</span><br><span class="line">                self.optimizer_c.zero_grad()</span><br><span class="line">                value_loss.backward()</span><br><span class="line">                nn.utils.clip_grad_norm_(self.cnet.parameters(), self.max_grad_norm)</span><br><span class="line">                self.optimizer_c.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># del self.buffer[:]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">flagVar = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">pendulumGoal = [<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rewardFunc</span>(<span class="params">goal, state, absMax</span>):</span></span><br><span class="line">    tmp = -(np.power(state[<span class="number">0</span>] - goal[<span class="number">0</span>], <span class="number">2</span>) + <span class="number">1</span>*np.power(state[<span class="number">1</span>] - goal[<span class="number">1</span>], <span class="number">2</span>) + <span class="number">0.1</span> * np.power(state[<span class="number">2</span>] - goal[<span class="number">2</span>], <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">return</span> (tmp + absMax)/absMax</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="keyword">global</span> flagVar</span><br><span class="line">    <span class="comment"># env = gym.make(&#x27;Pendulum-v1&#x27;, render_mode=&quot;human&quot;)</span></span><br><span class="line">    env = gym.make(<span class="string">&#x27;Pendulum-v1&#x27;</span>)</span><br><span class="line">    <span class="comment"># env.seed(args.seed)</span></span><br><span class="line">    <span class="comment"># env.render()</span></span><br><span class="line">    agent = Agent()</span><br><span class="line"></span><br><span class="line">    training_records = []</span><br><span class="line">    running_reward = -<span class="number">1000</span></span><br><span class="line">    <span class="comment"># state = env.reset()</span></span><br><span class="line">    TRAIN_EPISODE = <span class="number">1000</span></span><br><span class="line">    EPISODE_LENGTH = <span class="number">200</span></span><br><span class="line">    <span class="keyword">for</span> i_ep <span class="keyword">in</span> <span class="built_in">range</span>(TRAIN_EPISODE):</span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">        state = np.array(env.reset()[<span class="number">0</span>])</span><br><span class="line">        <span class="comment"># if i_ep % 100 == 0:</span></span><br><span class="line">        <span class="comment">#     print(&#x27;!&#x27;)</span></span><br><span class="line">        <span class="comment">#     env.render()</span></span><br><span class="line">        <span class="comment"># print(type(state))</span></span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">if</span> i_ep == <span class="number">0</span>:</span><br><span class="line">            firstY = []</span><br><span class="line">        <span class="keyword">if</span> i_ep == TRAIN_EPISODE-<span class="number">1</span>:</span><br><span class="line">            lastX = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(EPISODE_LENGTH)]</span><br><span class="line">            lastY = []</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(EPISODE_LENGTH):</span><br><span class="line">            action, action_log_prob = agent.select_action(state)</span><br><span class="line">            <span class="comment"># if i_ep%100 == 0:</span></span><br><span class="line">                <span class="comment"># print(&#x27;!&#x27;)</span></span><br><span class="line">                <span class="comment"># env.render()</span></span><br><span class="line">            envRet = env.step([action])</span><br><span class="line">            <span class="comment"># print(envRet)</span></span><br><span class="line">            state_, reward, done, _, _ = envRet</span><br><span class="line">            <span class="keyword">if</span> i_ep == TRAIN_EPISODE-<span class="number">1</span>:</span><br><span class="line">                lastY.append(math.acos(state_[<span class="number">0</span>]))</span><br><span class="line">                <span class="comment"># lastY.append(state_[0])</span></span><br><span class="line">            <span class="keyword">elif</span> i_ep == <span class="number">0</span>:</span><br><span class="line">                firstY.append(math.acos(state_[<span class="number">0</span>]))</span><br><span class="line">                <span class="comment"># firstY.append(state_[0])</span></span><br><span class="line">            <span class="comment"># print(state_)</span></span><br><span class="line">            <span class="comment"># if args.render:</span></span><br><span class="line">            <span class="comment">#     env.render()</span></span><br><span class="line">            <span class="keyword">if</span> agent.store(Transition(state, action, action_log_prob, (reward + <span class="number">8</span>) / <span class="number">8</span>, state_)):</span><br><span class="line">                agent.update()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># reward = rewardFunc(pendulumGoal, state_, 8)</span></span><br><span class="line">            <span class="comment"># if agent.store(Transition(state, action, action_log_prob, reward, state_)):</span></span><br><span class="line">            <span class="comment">#     agent.update()</span></span><br><span class="line"></span><br><span class="line">            score += reward</span><br><span class="line">            state = state_</span><br><span class="line"></span><br><span class="line">        running_reward = running_reward * <span class="number">0.85</span> + score * <span class="number">0.1</span></span><br><span class="line">        training_records.append(TrainingRecord(i_ep, running_reward))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i_ep % args.log_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Ep &#123;&#125;\tMoving average score: &#123;:.2f&#125;\t&#x27;</span>.<span class="built_in">format</span>(i_ep, running_reward))</span><br><span class="line">            time_ = time.time()</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;time used is &#123;:.5f&#125;&quot;</span>.<span class="built_in">format</span>(time_ - start))</span><br><span class="line">            <span class="comment"># start = time_</span></span><br><span class="line">        <span class="comment"># if running_reward &gt; -200:</span></span><br><span class="line">        <span class="comment">#     print(&quot;Solved! Moving average score is now &#123;&#125;!&quot;.format(running_reward))</span></span><br><span class="line">        <span class="comment">#     env.close()</span></span><br><span class="line">        <span class="comment">#     agent.save_param()</span></span><br><span class="line">        <span class="comment">#     with open(&#x27;log/ppo_training_records.pkl&#x27;, &#x27;wb&#x27;) as f:</span></span><br><span class="line">        <span class="comment">#         pickle.dump(training_records, f)</span></span><br><span class="line">        <span class="comment">#     break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># if running_reward &gt; -850 and not flagVar:  # 减小学习率防止波动</span></span><br><span class="line">        <span class="comment">#     flagVar = True</span></span><br><span class="line">        <span class="comment">#     print(&quot;-------------------------------&quot;)</span></span><br><span class="line">        <span class="comment">#     agent.optimizer_a = optim.Adam(agent.anet.parameters(), lr=1e-5)</span></span><br><span class="line">        <span class="comment">#     agent.optimizer_c = optim.Adam(agent.cnet.parameters(), lr=2e-5)</span></span><br><span class="line"></span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>, <span class="number">5</span>))</span><br><span class="line">    plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;SimHei&#x27;</span>]  <span class="comment"># 指定默认字体</span></span><br><span class="line">    plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>] = <span class="literal">False</span>  <span class="comment"># 解决保存图像时符号-显示为方块的2问题</span></span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    plt.plot([r.ep <span class="keyword">for</span> r <span class="keyword">in</span> training_records], [r.reward <span class="keyword">for</span> r <span class="keyword">in</span> training_records])</span><br><span class="line">    plt.title(<span class="string">&#x27;PPO&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Episode&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Moving averaged episode reward&#x27;</span>)</span><br><span class="line">    <span class="comment"># plt.savefig(&quot;img/ppo.png&quot;)</span></span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    plt.plot(lastX, firstY, label=<span class="string">&quot;第一次&quot;</span>)</span><br><span class="line">    plt.plot(lastX, lastY, label=<span class="string">&quot;最后一次&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;cos(&quot;</span>+<span class="string">r&#x27;$\theta$&#x27;</span>+<span class="string">&quot;)&quot;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&quot;time&quot;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br><span class="line">    <span class="comment"># example = [i for i in range(100)]</span></span><br><span class="line">    <span class="comment"># print(len([i for i in BatchSampler(SubsetRandomSampler(range(100)), 20, False)]))</span></span><br></pre></td></tr></table></figure>
<ul>
<li>效果<img src="/imgs/d0ba034ff53840a354c441cf51e2eacc8194cbbd0fffe8f9ec9d557d97fe54ee.png" alt="picture 4">  </li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-HindsightExperienceReplay（HER）"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/10/13/HindsightExperienceReplay%EF%BC%88HER%EF%BC%89/"
    >HindsightExperienceReplay（HER）</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/13/HindsightExperienceReplay%EF%BC%88HER%EF%BC%89/" class="article-date">
  <time datetime="2022-10-13T06:45:00.000Z" itemprop="datePublished">2022-10-13</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="Hindsight-Experience-Replay"><a href="#Hindsight-Experience-Replay" class="headerlink" title="Hindsight Experience Replay"></a>Hindsight Experience Replay</h1><ul>
<li>原文<ul>
<li>Hindsight experience replay</li>
<li>Advances in neural information processing systems</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43145941/article/details/119219436?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-119219436-blog-79498248.pc_relevant_3mothn_strategy_and_data_recovery&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2~default~CTRLIST~Rate-1-119219436-blog-79498248.pc_relevant_3mothn_strategy_and_data_recovery&utm_relevant_index=2">参考链接</a><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2></li>
<li>你得知道从状态S到目标goal的映射关系（以机械臂为例，状态可能是多个关节的角度，目标是三维空间一个点的坐标，如果知道状态，那么也能推算出机械臂末端在空间中的坐标）；</li>
<li>你得建立一个新的reward计算机制，它取决于目标goal和状态S，一般当状态S映射的goal’与goal相近时给予奖励；</li>
<li>你得创建一个记录每个episode transition的列表，它的作用是在每个episode结束后进行事后经验回放，具体回放方法之后讲；</li>
<li>RL算法接受的状态维度相较于原始的维度增加了目标goal的维度，也就是RL接受：<ul>
<li><img src="/imgs/dba2e2146bcea81c1cc62e3cafac2015a03a4b67fb2a29916283b8ea130ea278.png" alt="picture 2">  <h2 id="举例的github"><a href="#举例的github" class="headerlink" title="举例的github"></a>举例的github</h2></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ZYunfeii/DRL_algorithm_library/tree/master/DDPG/DDPG_spinningup_HER">HER举例</a></li>
<li>这里episode_cache为存储transition的列表((s1,a1,r1,s1’),(s2,a2,r2,s2’)…)，枚举这个列表的元素。在第二句根据每个transition产生HER_SAMPLE_NUM个新的目标点new_goals，这些<strong>目标点时根据之后的transition的state推算得到的</strong>，当然一种简单的情况就是state。之后对这些new_goals做遍历，对每一个new_goal都重新计算reward，并将transition中s和s’的goal部分替换为new_goal，之后将这个新的transition存储入经验池buffer。这里之所以可以这么做是因为在动作a不变的情况下，改变goal是不会改变从原来的s转移到s’的转移概率的。</li>
<li>先对每个回合中的所有输入做一个reward的评价</li>
<li>然后在整个回合的数据后处理时，循环到i时，从i之后的数据里随机选出一部分作为新的goal，然后利用这些新的goal重新计算i这个数据的reward，然后将其放入到replay buffer中，可能导致replay buffer中包含多条由同一条数据而来但是reward不同的数据条目</li>
<li>可以在每训练一个回合之后更换初始的goal（也就是在筛选之前针对所有对象的goal）达到多目标训练的效果</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ZYunfeii/DRL_algorithm_library/blob/master/DDPG/DDPG_spinningup_HER/main.py">示例代码</a></li>
<li><img src="/imgs/000d30cf3c120ce06399c7b098681c9274e60a801893efe32854f09bf18d13d6.png" alt="picture 5">  </li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-强化学习复习（四）"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/10/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89/"
    >强化学习复习（四）</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/09/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%EF%BC%88%E5%9B%9B%EF%BC%89/" class="article-date">
  <time datetime="2022-10-09T07:27:06.000Z" itemprop="datePublished">2022-10-09</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/sweetice/Deep-reinforcement-learning-with-pytorch">基于pytorch实现的强化学习网络</a></p>
<ul>
<li><strong>这个的PPO写的似乎有问题</strong></li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/labmlai/annotated_deep_learning_paper_implementations">强化学习网络集合</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/thu-ml/tianshou">集合 2</a></p>
</li>
<li><h2 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h2></li>
<li><p>PPO是基于Actor-Critic的算法</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/468828804">参考链接</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> Normal</span><br><span class="line"><span class="keyword">from</span> torch.utils.data.sampler <span class="keyword">import</span> BatchSampler, SubsetRandomSampler</span><br><span class="line"></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;Solve the Pendulum-v0 with PPO&#x27;</span>)</span><br><span class="line">parser.add_argument(</span><br><span class="line">    <span class="string">&#x27;--gamma&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.9</span>, metavar=<span class="string">&#x27;G&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;discount factor (default: 0.9)&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">0</span>, metavar=<span class="string">&#x27;N&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;random seed (default: 0)&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--render&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, <span class="built_in">help</span>=<span class="string">&#x27;render the environment&#x27;</span>)</span><br><span class="line">parser.add_argument(</span><br><span class="line">    <span class="string">&#x27;--log-interval&#x27;</span>,</span><br><span class="line">    <span class="built_in">type</span>=<span class="built_in">int</span>,</span><br><span class="line">    default=<span class="number">10</span>,</span><br><span class="line">    metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">    <span class="built_in">help</span>=<span class="string">&#x27;interval between training status logs (default: 10)&#x27;</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">torch.manual_seed(args.seed)</span><br><span class="line"></span><br><span class="line">TrainingRecord = namedtuple(<span class="string">&#x27;TrainingRecord&#x27;</span>, [<span class="string">&#x27;ep&#x27;</span>, <span class="string">&#x27;reward&#x27;</span>])</span><br><span class="line">Transition = namedtuple(<span class="string">&#x27;Transition&#x27;</span>, [<span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;a_log_p&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s_&#x27;</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ActorNet, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(<span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line">        self.mu_head = nn.Linear(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">        self.sigma_head = nn.Linear(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.fc(x))</span><br><span class="line">        mu = <span class="number">2.0</span> * F.tanh(self.mu_head(x))</span><br><span class="line">        sigma = F.softplus(self.sigma_head(x))</span><br><span class="line">        <span class="keyword">return</span> (mu, sigma)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CriticNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CriticNet, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(<span class="number">3</span>, <span class="number">100</span>)</span><br><span class="line">        self.v_head = nn.Linear(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(self.fc(x))</span><br><span class="line">        state_value = self.v_head(x)</span><br><span class="line">        <span class="keyword">return</span> state_value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>():</span></span><br><span class="line"></span><br><span class="line">    clip_param = <span class="number">0.2</span></span><br><span class="line">    max_grad_norm = <span class="number">0.5</span></span><br><span class="line">    ppo_epoch = <span class="number">10</span></span><br><span class="line">    buffer_capacity, batch_size = <span class="number">1000</span>, <span class="number">32</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.training_step = <span class="number">0</span></span><br><span class="line">        self.anet = ActorNet().<span class="built_in">float</span>()</span><br><span class="line">        self.cnet = CriticNet().<span class="built_in">float</span>()</span><br><span class="line">        self.buffer = []</span><br><span class="line">        self.counter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        self.optimizer_a = optim.Adam(self.anet.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line">        self.optimizer_c = optim.Adam(self.cnet.parameters(), lr=<span class="number">3e-4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">self, state</span>):</span></span><br><span class="line">        state = torch.from_numpy(state).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            (mu, sigma) = self.anet(state)</span><br><span class="line">        dist = Normal(mu, sigma)</span><br><span class="line">        action = dist.sample()</span><br><span class="line">        action_log_prob = dist.log_prob(action)</span><br><span class="line">        action = action.clamp(-<span class="number">2.0</span>, <span class="number">2.0</span>)</span><br><span class="line">        <span class="keyword">return</span> action.item(), action_log_prob.item()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_value</span>(<span class="params">self, state</span>):</span></span><br><span class="line"></span><br><span class="line">        state = torch.from_numpy(state).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            state_value = self.cnet(state)</span><br><span class="line">        <span class="keyword">return</span> state_value.item()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save_param</span>(<span class="params">self</span>):</span></span><br><span class="line">        torch.save(self.anet.state_dict(), <span class="string">&#x27;param/ppo_anet_params.pkl&#x27;</span>)</span><br><span class="line">        torch.save(self.cnet.state_dict(), <span class="string">&#x27;param/ppo_cnet_params.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">store</span>(<span class="params">self, transition</span>):</span></span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line">        self.counter += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.counter % self.buffer_capacity == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.training_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        s = torch.tensor([t.s <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">        a = torch.tensor([t.a <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        r = torch.tensor([t.r <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        s_ = torch.tensor([t.s_ <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">        old_action_log_probs = torch.tensor(</span><br><span class="line">            [t.a_log_p <span class="keyword">for</span> t <span class="keyword">in</span> self.buffer], dtype=torch.<span class="built_in">float</span>).view(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        r = (r - r.mean()) / (r.std() + <span class="number">1e-5</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            target_v = r + args.gamma * self.cnet(s_)</span><br><span class="line"></span><br><span class="line">        adv = (target_v - self.cnet(s)).detach()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.ppo_epoch):</span><br><span class="line">            <span class="keyword">for</span> index <span class="keyword">in</span> BatchSampler(</span><br><span class="line">                    SubsetRandomSampler(<span class="built_in">range</span>(self.buffer_capacity)), self.batch_size, <span class="literal">False</span>):</span><br><span class="line"></span><br><span class="line">                (mu, sigma) = self.anet(s[index])</span><br><span class="line">                dist = Normal(mu, sigma)</span><br><span class="line">                action_log_probs = dist.log_prob(a[index])</span><br><span class="line">                ratio = torch.exp(action_log_probs - old_action_log_probs[index])</span><br><span class="line"></span><br><span class="line">                surr1 = ratio * adv[index]</span><br><span class="line">                surr2 = torch.clamp(ratio, <span class="number">1.0</span> - self.clip_param,</span><br><span class="line">                                    <span class="number">1.0</span> + self.clip_param) * adv[index]</span><br><span class="line">                action_loss = -torch.<span class="built_in">min</span>(surr1, surr2).mean()</span><br><span class="line"></span><br><span class="line">                self.optimizer_a.zero_grad()</span><br><span class="line">                action_loss.backward()</span><br><span class="line">                nn.utils.clip_grad_norm_(self.anet.parameters(), self.max_grad_norm)</span><br><span class="line">                self.optimizer_a.step()</span><br><span class="line"></span><br><span class="line">                value_loss = F.smooth_l1_loss(self.cnet(s[index]), target_v[index])</span><br><span class="line">                self.optimizer_c.zero_grad()</span><br><span class="line">                value_loss.backward()</span><br><span class="line">                nn.utils.clip_grad_norm_(self.cnet.parameters(), self.max_grad_norm)</span><br><span class="line">                self.optimizer_c.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">del</span> self.buffer[:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    env = gym.make(<span class="string">&#x27;Pendulum-v0&#x27;</span>)</span><br><span class="line">    env.seed(args.seed)</span><br><span class="line"></span><br><span class="line">    agent = Agent()</span><br><span class="line"></span><br><span class="line">    training_records = []</span><br><span class="line">    running_reward = -<span class="number">1000</span></span><br><span class="line">    state = env.reset()</span><br><span class="line">    <span class="keyword">for</span> i_ep <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">        score = <span class="number">0</span></span><br><span class="line">        state = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):</span><br><span class="line">            action, action_log_prob = agent.select_action(state)</span><br><span class="line">            state_, reward, done, _ = env.step([action])</span><br><span class="line">            <span class="keyword">if</span> args.render:</span><br><span class="line">                env.render()</span><br><span class="line">            <span class="keyword">if</span> agent.store(Transition(state, action, action_log_prob, (reward + <span class="number">8</span>) / <span class="number">8</span>, state_)):</span><br><span class="line">                agent.update()</span><br><span class="line">            score += reward</span><br><span class="line">            state = state_</span><br><span class="line"></span><br><span class="line">        running_reward = running_reward * <span class="number">0.9</span> + score * <span class="number">0.1</span></span><br><span class="line">        training_records.append(TrainingRecord(i_ep, running_reward))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i_ep % args.log_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Ep &#123;&#125;\tMoving average score: &#123;:.2f&#125;\t&#x27;</span>.<span class="built_in">format</span>(i_ep, running_reward))</span><br><span class="line">        <span class="keyword">if</span> running_reward &gt; -<span class="number">200</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Solved! Moving average score is now &#123;&#125;!&quot;</span>.<span class="built_in">format</span>(running_reward))</span><br><span class="line">            env.close()</span><br><span class="line">            agent.save_param()</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;log/ppo_training_records.pkl&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                pickle.dump(training_records, f)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    plt.plot([r.ep <span class="keyword">for</span> r <span class="keyword">in</span> training_records], [r.reward <span class="keyword">for</span> r <span class="keyword">in</span> training_records])</span><br><span class="line">    plt.title(<span class="string">&#x27;PPO&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Episode&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Moving averaged episode reward&#x27;</span>)</span><br><span class="line">    plt.savefig(<span class="string">&quot;img/ppo.png&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></li>
<li><p>产生动作的方式与一般的PG类似，就是使用PG输出一个μ和σ然后使用正态分布，然后进行采样输出真正的动作</p>
<h3 id="网络更新"><a href="#网络更新" class="headerlink" title="网络更新"></a>网络更新</h3></li>
<li><p>reward归一化，将reward-平均数/标准差</p>
</li>
<li><p>计算target价值，使用当前回合的reward+系数×Critic对于下一状态的分析</p>
</li>
<li><p>估计价值和实际价值的差：直接用估计当前状态的价值与上一步计算的价值做差</p>
</li>
<li><p>计算actor网络的loss的方法是</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">(mu, sigma) = self.anet(s[index])</span><br><span class="line">dist = Normal(mu, sigma)</span><br><span class="line">action_log_probs = dist.log_prob(a[index])</span><br><span class="line">ratio = torch.exp(action_log_probs - old_action_log_probs[index])</span><br><span class="line"></span><br><span class="line">surr1 = ratio * adv[index]</span><br><span class="line">surr2 = torch.clamp(ratio, <span class="number">1.0</span> - self.clip_param,</span><br><span class="line">                    <span class="number">1.0</span> + self.clip_param) * adv[index]</span><br><span class="line">action_loss = -torch.<span class="built_in">min</span>(surr1, surr2).mean()</span><br></pre></td></tr></table></figure></li>
<li><p>先利用当前的网络采样输出μ和σ，然后计算之前action的log_prob，，然后利用表达式<code>e^(这次网络计算出的log可能性-上次网络计算出的log可能性)</code>，实际上就是<code>这次网络计算出的可能性/上次网络计算出的可能性</code>与上一步算出的估计价值和实际价值的差值<strong>修正这个值</strong>，主要目的是用合理的方法<u>利用其他时刻动作的输出，增加一个动作的利用效率</u></p>
</li>
<li><p><img src="/imgs/f8606933037c3541da1fdedf4142d2a36bcb61ee2ddd0dba8b63febd43dfd75c.png" alt="picture 1">  </p>
</li>
<li><p>然后用一个参数×上面的数值，处理之后得到最终需要反向传播的值（actor）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">value_loss = F.smooth_l1_loss(self.cnet(s[index]), target_v[index])</span><br><span class="line">self.optimizer_c.zero_grad()</span><br><span class="line">value_loss.backward()</span><br><span class="line">nn.utils.clip_grad_norm_(self.cnet.parameters(), self.max_grad_norm)</span><br><span class="line">self.optimizer_c.step()</span><br></pre></td></tr></table></figure></li>
<li><p>更新Critic的参数操作是将现在的Critic网络对价值的判断与前面算出的目标价值判断进行比较，将二者的差进行反向传播</p>
<h2 id="BatchSampler和SubsetRamdomSampler"><a href="#BatchSampler和SubsetRamdomSampler" class="headerlink" title="BatchSampler和SubsetRamdomSampler"></a>BatchSampler和SubsetRamdomSampler</h2></li>
<li><p><code>SubsetRandomSampler</code>实际上是将数据的顺序打乱做一个全排列</p>
</li>
<li><p><code>BatchSampler</code>实际上是根据设置的batch_size给数据分成一个个的batch</p>
</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-Anaconda配置问题记录"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/10/09/Anaconda%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"
    >Anaconda配置问题记录</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/09/Anaconda%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/" class="article-date">
  <time datetime="2022-10-09T06:25:18.000Z" itemprop="datePublished">2022-10-09</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="找不到-condarc文件"><a href="#找不到-condarc文件" class="headerlink" title="找不到.condarc文件"></a>找不到<code>.condarc</code>文件</h2><ul>
<li>先执行一次<code>conda config</code>就能找到了<h2 id="配置Anaconda使用clash代理"><a href="#配置Anaconda使用clash代理" class="headerlink" title="配置Anaconda使用clash代理"></a>配置Anaconda使用clash代理</h2></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43135178/article/details/124499250">链接</a><h2 id="修改Anaconda安装环境的默认位置"><a href="#修改Anaconda安装环境的默认位置" class="headerlink" title="修改Anaconda安装环境的默认位置"></a>修改Anaconda安装环境的默认位置</h2></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36455412/article/details/125347552">链接</a><h2 id="手动添加Anaconda到环境变量"><a href="#手动添加Anaconda到环境变量" class="headerlink" title="手动添加Anaconda到环境变量"></a>手动添加Anaconda到环境变量</h2></li>
<li><a target="_blank" rel="noopener" href="http://news.sohu.com/a/446583263_120918998">链接</a><h2 id="配置pytorch使用CPU的多线程"><a href="#配置pytorch使用CPU的多线程" class="headerlink" title="配置pytorch使用CPU的多线程"></a>配置pytorch使用CPU的多线程</h2></li>
<li><code>torch.set_num_threads(8)</code></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/a_piece_of_ppx/article/details/123714865">参考</a></li>
<li>作用不大<h2 id="GYM-官方文档"><a href="#GYM-官方文档" class="headerlink" title="GYM 官方文档"></a>GYM 官方文档</h2></li>
<li><a target="_blank" rel="noopener" href="https://www.gymlibrary.dev/">链接</a><h2 id="word中插入代码"><a href="#word中插入代码" class="headerlink" title="word中插入代码"></a>word中插入代码</h2></li>
<li><a target="_blank" rel="noopener" href="https://highlightcode.com/">https://highlightcode.com/</a></li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/anaconda/" rel="tag">anaconda</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-强化学习复习（三）"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2022/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/"
    >强化学习复习（三）</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2022/10/07/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%EF%BC%88%E4%B8%89%EF%BC%89/" class="article-date">
  <time datetime="2022-10-07T13:12:05.000Z" itemprop="datePublished">2022-10-07</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <ul>
<li><p><a target="_blank" rel="noopener" href="https://github.com/pytorch/examples/tree/main/reinforcement_learning">pytorch关于强化学习的示例</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/_modules/torch/distributions/normal.html">pytorch源码</a></p>
<h2 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h2></li>
<li><p>基本上就是通过动作获得的奖励或者惩罚信息反向传播，给<code>Actor</code>网络进行指导</p>
</li>
<li><p><code>Critic</code>实际上是一个类似于<code>QNetwork</code>的网络，它的作用是对Actor的动作做出每个时刻的评价，之前只能在回合结束的时候根据给出的回报进行更新，但是拥有<code>Critic</code>之后就可以在每个时刻进行更新了，也就是**在一个回合结束之前，猜测出这个动作可能导致的reward，并以此指导<code>Actor</code>**。</p>
</li>
<li><p>例子</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> count</span><br><span class="line"> </span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.distributions <span class="keyword">import</span> Categorical</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;PyTorch REINFORCE example&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--gamma&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.99</span>, metavar=<span class="string">&#x27;G&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;discount factor (default: 0.99)&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">543</span>, metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;random seed (default: 543)&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--render&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;render the environment&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--log-interval&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10</span>, metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">                    <span class="built_in">help</span>=<span class="string">&#x27;interval between training status logs (default: 10)&#x27;</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">env = gym.make(<span class="string">&#x27;CartPole-v1&#x27;</span>)</span><br><span class="line">env.seed(args.seed)</span><br><span class="line">torch.manual_seed(args.seed)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Policy</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Policy, self).__init__()</span><br><span class="line">        self.affine1 = nn.Linear(<span class="number">4</span>, <span class="number">128</span>)</span><br><span class="line">        self.dropout = nn.Dropout(p=<span class="number">0.6</span>)</span><br><span class="line">        self.affine2 = nn.Linear(<span class="number">128</span>, <span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">        self.saved_log_probs = []</span><br><span class="line">        self.rewards = []</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.affine1(x)</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        action_scores = self.affine2(x)</span><br><span class="line">        <span class="keyword">return</span> F.softmax(action_scores, dim=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">policy = Policy()</span><br><span class="line">optimizer = optim.Adam(policy.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line">eps = np.finfo(np.float64).eps.item()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span>(<span class="params">state</span>):</span></span><br><span class="line">    state = torch.from_numpy(state).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>)</span><br><span class="line">    probs = policy(state)</span><br><span class="line">    m = Categorical(probs)</span><br><span class="line">    action = m.sample()</span><br><span class="line">    policy.saved_log_probs.append(m.log_prob(action))</span><br><span class="line">    <span class="keyword">return</span> action.item()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">finish_episode</span>():</span></span><br><span class="line">    R = <span class="number">0</span></span><br><span class="line">    policy_loss = []</span><br><span class="line">    returns = []</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> policy.rewards[::-<span class="number">1</span>]:</span><br><span class="line">        R = r + args.gamma * R</span><br><span class="line">        returns.insert(<span class="number">0</span>, R)</span><br><span class="line">    returns = torch.tensor(returns)</span><br><span class="line">    returns = (returns - returns.mean()) / (returns.std() + eps)</span><br><span class="line">    <span class="keyword">for</span> log_prob, R <span class="keyword">in</span> <span class="built_in">zip</span>(policy.saved_log_probs, returns):</span><br><span class="line">        policy_loss.append(-log_prob * R)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    policy_loss = torch.cat(policy_loss).<span class="built_in">sum</span>()</span><br><span class="line">    policy_loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="keyword">del</span> policy.rewards[:]</span><br><span class="line">    <span class="keyword">del</span> policy.saved_log_probs[:]</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    running_reward = <span class="number">10</span></span><br><span class="line">    <span class="keyword">for</span> i_episode <span class="keyword">in</span> count(<span class="number">1</span>):</span><br><span class="line">        state, ep_reward = env.reset(), <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10000</span>):  <span class="comment"># Don&#x27;t infinite loop while learning</span></span><br><span class="line">            action = select_action(state)</span><br><span class="line">            state, reward, done, _ = env.step(action)</span><br><span class="line">            <span class="keyword">if</span> args.render:</span><br><span class="line">                env.render()</span><br><span class="line">            policy.rewards.append(reward)</span><br><span class="line">            ep_reward += reward</span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"> </span><br><span class="line">        running_reward = <span class="number">0.05</span> * ep_reward + (<span class="number">1</span> - <span class="number">0.05</span>) * running_reward</span><br><span class="line">        finish_episode()</span><br><span class="line">        <span class="keyword">if</span> i_episode % args.log_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Episode &#123;&#125;\tLast reward: &#123;:.2f&#125;\tAverage reward: &#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                  i_episode, ep_reward, running_reward))</span><br><span class="line">        <span class="keyword">if</span> running_reward &gt; env.spec.reward_threshold:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Solved! Running reward is now &#123;&#125; and &quot;</span></span><br><span class="line">                  <span class="string">&quot;the last episode runs to &#123;&#125; time steps!&quot;</span>.<span class="built_in">format</span>(running_reward, t))</span><br><span class="line">            torch.save(policy.state_dict(),<span class="string">&#x27;hello.pt&#x27;</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></li>
<li><p><code>Categorical</code>：<img src="/imgs/bbff369c31599485cece0ecd1f86a3c2d5f7d254eeb1b8f76de5053a207275b2.png" alt="picture 49">  </p>
<ul>
<li><code>log_probs</code><img src="/imgs/b02e3510312ce797463f7218ef7fde86af78b655a54491b1bee2766aac44d92d.png" alt="picture 50">  ，实际上就是将对应动作发生的可能性求了log<h3 id="给出策略的具体操作"><a href="#给出策略的具体操作" class="headerlink" title="给出策略的具体操作"></a>给出策略的具体操作</h3></li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/439220435">参考链接</a></p>
</li>
<li><p>给出策略的具体操作是先将输入经过一系列网络的运算之后，经过softmax归一化，得到和为1的几个输出。然后在输出过程中对于具有这几种概率的输出进行随机取样，得到最终的输出动作。（<strong>离散动作</strong>）</p>
</li>
<li><p>针对<strong>连续动作</strong>，可以将整个网络的输出更改为输出一个高斯分布函数的μ值（均值），结合用户指定的σ（方差），即可形成一个高斯分布，然后通过类似的sample采样即可得出需要的动作。注意训练阶段为了实现有效的exploration，不要使用太小的σ，否则因为输出太集中没法找到实际上的最优解。</p>
<ul>
<li>也可以让网络也输出σ</li>
<li>反向传播的思路相似，也是直接利用<code>torch.Distributions.Normal</code>的<code>log_prob</code>函数输出概率的log值<img src="/imgs/da881cab2fe598db70c03731a3ec37aec64f13d6f84d04168d697c1aa1d390d6.png" alt="picture 51"></li>
</ul>
</li>
</ul>
<h3 id="网络更新"><a href="#网络更新" class="headerlink" title="网络更新"></a>网络更新</h3><ul>
<li><img src="/imgs/b734753c9daa77b5104e2738198288151b8eafd8ec96a44556debd73f729b630.png" alt="picture 48">  <h2 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h2></li>
<li>现在我们来说说 DDPG 中所用到的神经网络. 它其实和我们之前提到的 Actor-Critic 形式差不多, 也需要有基于 策略 Policy 的神经网络 和基于 价值 Value 的神经网络, 但是为了体现 DQN 的思想, 每种神经网络我们都需要再细分为两个, Policy Gradient 这边, 我们有估计网络和现实网络, 估计网络用来输出实时的动作, 供 actor 在现实中实行. 而现实网络则是用来更新价值网络系统的. 所以我们再来看看价值系统这边, 我们也有现实网络和估计网络, 他们都在输出这个状态的价值, 而输入端却有不同, <strong>状态现实网络这边会拿着从动作现实网络来的动作加上状态的观测值加以分析, 而状态估计网络则是拿着当时 Actor 施加的动作当做输入</strong>.在实际运用中, DDPG 的这种做法的确带来了更有效的学习过程.</li>
<li><img src="/imgs/6a83e078a6bdb97f3a4cd97e1da8d01ea4068866f7bf6254e44e27d51c3f03ea.png" alt="picture 43">  <h3 id="学习的过程"><a href="#学习的过程" class="headerlink" title="学习的过程"></a>学习的过程</h3><h4 id="Critic网络"><a href="#Critic网络" class="headerlink" title="Critic网络"></a>Critic网络</h4></li>
<li><code>y_true</code>是要学习的值，这个值是通过<code>Critci</code>的<code>target</code>网络对于下一时刻的<code>actor</code>的<code>target</code>网络的动作做出的评估加上这一时刻的汇报<code>reward</code>计算出来的，而它自身需要修改的值就是直接对当前的环境观测和动作做出的值的判断<code>y_pred</code><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">critic_learn</span>():</span></span><br><span class="line">    a1 = self.actor_target(s1).detach()</span><br><span class="line">    y_true = r1 + self.gamma * self.critic_target(s1, a1).detach()</span><br><span class="line">    </span><br><span class="line">    y_pred = self.critic(s0, a0)</span><br><span class="line">    </span><br><span class="line">    loss_fn = nn.MSELoss()</span><br><span class="line">    loss = loss_fn(y_pred, y_true)</span><br><span class="line">    self.critic_optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    self.critic_optim.step()</span><br></pre></td></tr></table></figure></li>
<li>然后按照一定的比例<code>soft_update</code>对应的<code>target</code>网络即可<h4 id="Actor网络"><a href="#Actor网络" class="headerlink" title="Actor网络"></a>Actor网络</h4></li>
<li>直接利用<code>critic</code>网络对于此刻环境的观测和在此刻环境下<code>actor</code>网络的行为做出评价，然后直接反向传播</li>
<li>同样<code>soft_update</code>另一个<code>target</code>网络即可<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">actor_learn</span>():</span></span><br><span class="line">    loss = -torch.mean( self.critic(s0, self.actor(s0)) )</span><br><span class="line">    self.actor_optim.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    self.actor_optim.step()</span><br></pre></td></tr></table></figure>
<h2 id="A3C"><a href="#A3C" class="headerlink" title="A3C"></a>A3C</h2></li>
<li><a target="_blank" rel="noopener" href="https://github.com/MorvanZhou/pytorch-A3C">pytorch A3C参考</a></li>
<li><img src="/imgs/8bed5783887792cdbaffbae4046098a68b2539e3a7093d5a4624fb6bb1378a00.png" alt="picture 44">  </li>
<li><img src="/imgs/bf942436af1a670b0ab91f0705dd4a8136963ade316c598d29a6a775f107841f.png" alt="picture 45">  </li>
<li><img src="/imgs/daf4381936a5297b7e62f97c553d7c997f51b7ebbecfaee2b4b204c1dd156fba.png" alt="picture 46">  </li>
<li><img src="/imgs/1cdcd316476945548933f8ad5922351079bb5ee5808860489a251970aa3c805d.png" alt="picture 47">  </li>
<li>实际上每个本地网络都是一个Actor-Critic的网络，损失分为动作网络<code>Actor</code>的loss和<code>Critic</code>网络的loss<ul>
<li>Critic的loss可以先计算<code>td_error</code>，用Critic在此时的环境中计算出的值与实际上每一步得到的增加随时间衰减的因子之后的实际上的Reward做差，然后平方即可得到Critic的loss</li>
<li>Actor的loss则是使用反向求出刚才动作的log_prob（怎么求上文有），然后再求出<code>entropy</code>，公式为<img src="/imgs/082feca831bedf90bf6c3253215c0e01b8c26b2ba406997eb4acb361db659f73.png" alt="`0.5 + 0.5 * math.log(2 * math.pi) + torch.log(m.scale)` 1">，然后log_prob×上文的td_error+一个系数×entropy，然后整个计算出来之后取相反数即可得到Actor的loss，然后将整个的两个loss取平均数，反向传播更新参数即可<u>（因为根据计算图倒推可以分别得到组成这个变量的两个变量分别的影响因素，所以不影响反向传播分别更新两个网络）</u>。<h3 id="torch中backword是怎么用的"><a href="#torch中backword是怎么用的" class="headerlink" title="torch中backword是怎么用的"></a>torch中backword是怎么用的</h3></li>
</ul>
</li>
<li>针对<strong>标量</strong>做出的对计算图的反向传播，得到标量的值，算出计算图中每个变量对于得到这个标量的偏导数<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/168748668">参考</a></li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/4/">上一页</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="page-number" href="/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/page/16/">16</a><a class="extend next" rel="next" href="/page/6/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2021-2023
        <i class="ri-heart-fill heart_icon"></i> FrankZhang
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/avatar.png" alt="Frank’s blogs"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>