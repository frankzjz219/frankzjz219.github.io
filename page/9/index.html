<!DOCTYPE html><html lang="Chinese"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content=""><meta name="keywords" content=""><meta name="author" content="FrankZhang"><meta name="copyright" content="FrankZhang"><title>Frank’s blogs</title><link rel="shortcut icon" href="/Flogo.png"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '5.4.0'
} </script><meta name="generator" content="Hexo 5.4.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="author-info"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">FrankZhang</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">84</span></a></div></div></div><nav class="no-bg" id="nav"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Frank’s blogs</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/archives">Archives</a></span><span class="pull-right"></span></div><div id="site-info"><div id="site-title">Frank’s blogs</div><div id="site-sub-title"></div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item article-container"><a class="article-title" href="/2021/12/11/FreeRtos1/">FreeRtos1</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-12-11</time><div class="content"><h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="资源连接集合"><a href="#资源连接集合" class="headerlink" title="资源连接集合"></a>资源连接集合</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/97869854">ARM+LINUX嵌入式学习路线 - 知乎 (zhihu.com)</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://www.openedv.com/docs/book-videos/zdyzshipin/4free/zdyz-freertos-book.html">http://www.openedv.com/docs/book-videos/zdyzshipin/4free/zdyz-freertos-book.html</a></p>
</li>
</ul>
<h2 id="为什么选择FreeRtos"><a href="#为什么选择FreeRtos" class="headerlink" title="为什么选择FreeRtos?"></a>为什么选择FreeRtos?</h2><ul>
<li><p>RTOS 类系统有很多，比如 uC/OS，资料很多，尤其是中文资料，那为什么要选择 FreeRTOS 呢？</p>
</li>
<li><p>FreeRTOS是免费的，学习RTOS操作系统的话 uC/OS是首选，但要做产品的话，免费的FreeRTOS操作系统就是个不错的选择。</p>
</li>
<li><p>许多半导体厂商产品的 SDK(Software Development Kit—软件开发工具包) 包就使用 FreeRTOS 作为其操作系统，尤其是 WIFI、蓝牙这些带协议栈的芯片或模块。</p>
</li>
<li><p>简单，因为FreeRTOS 的文件数量很少。</p>
</li>
</ul>
<h2 id="FreeRtos-特点"><a href="#FreeRtos-特点" class="headerlink" title="FreeRtos 特点"></a>FreeRtos 特点</h2><ul>
<li>FreeRTOS 的内核支持抢占式，合作式和时间片调度。</li>
<li>提供了一个用于低功耗的 Tickless 模式。</li>
<li>系统的组件在创建时可以选择动态或者静态的 RAM，比如任务、消息队列、信号量、软件定时器等等。</li>
<li>FreeRTOS-MPU 支持 Corex-M 系列中的 MPU 单元，如 STM32F429。</li>
<li>FreeRTOS 系统简单、小巧、易用，通常情况下内核占用 4k-9k 字节的空间。</li>
<li>高可移植性，代码主要 C 语言编写。</li>
<li>高效的软件定时器。</li>
<li>强大的跟踪执行功能。</li>
<li>堆栈溢出检测功能。</li>
<li>任务数量不限。</li>
<li>任务优先级不限。</li>
</ul>
<p><strong>官网：</strong><a target="_blank" rel="noopener" href="http://www.freertos.org/">www.freertos.org</a></p>
<h2 id="基于STM32F407和Cubemx（HAL库）的FreeRtos开发"><a href="#基于STM32F407和Cubemx（HAL库）的FreeRtos开发" class="headerlink" title="基于STM32F407和Cubemx（HAL库）的FreeRtos开发"></a>基于STM32F407和Cubemx（HAL库）的FreeRtos开发</h2><ul>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/90608412">FreeRTOS 从入门到精通1–实时操作系统的前世今生 - 知乎 (zhihu.com)</a></p>
</li>
<li><p>为什么选用HAl库？因为ST官方 <strong><u>已经停止</u></strong> 对于标准库的维护</p>
</li>
<li><p>使用STM32CubeIDE的（ <strong>未尝试盗版仿真器能不能使用</strong> ）</p>
</li>
</ul>
<h2 id="待续"><a href="#待续" class="headerlink" title="待续"></a>待续</h2></div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/12/08/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/">pytorch入门笔记</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-12-08</time><div class="content"><h1 id="pytorch入门笔记及资源集合"><a href="#pytorch入门笔记及资源集合" class="headerlink" title="pytorch入门笔记及资源集合"></a>pytorch入门笔记及资源集合</h1><h2 id="教程手册"><a href="#教程手册" class="headerlink" title="教程手册"></a>教程手册</h2><p>莫烦 <a target="_blank" rel="noopener" href="https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/">https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/</a></p>
<p>pytorch中文手册</p>
<p><a target="_blank" rel="noopener" href="https://handbook.pytorch.wiki/">https://handbook.pytorch.wiki/</a></p>
<h2 id="转移到GPU上训练"><a href="#转移到GPU上训练" class="headerlink" title="转移到GPU上训练"></a>转移到GPU上训练</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_18644873/article/details/86579147">https://blog.csdn.net/qq_18644873/article/details/86579147</a></p>
<p>CPU代码转换为GPU代码的主要操作就是利用</p>
<p><code>.cuda()</code></p>
<p>实现变量迁移到GPU中</p>
<p>torch变量后直接加cuda</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s0 = torch.tensor(s0, dtype=torch.<span class="built_in">float</span>).cuda()</span><br></pre></td></tr></table></figure>

<p>假如一个类继承了<code>nn.Module</code>类的话，直接调用这个类的<code>.cuda()</code></p>
<p>比如某个类定义如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(input_size, hidden_size)</span><br><span class="line">        self.linear2 = nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        self.linear3 = nn.Linear(hidden_size, output_size)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, s</span>):</span></span><br><span class="line">    x = F.relu(self.linear1(s))</span><br><span class="line">    x = F.relu(self.linear2(x))</span><br><span class="line">    x = torch.tanh(self.linear3(x))</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>转换为GPU的方法是</p>
<p><code>self.critic = Critic(s_dim + a_dim, 256, 1).cuda()</code><br>注意某些变量在转化为numpy数组之前必须用.cpu()拷贝回CPU才行</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s0 = torch.tensor(s0, dtype=torch.<span class="built_in">float</span>).cuda().unsqueeze(<span class="number">0</span>).cuda()  <span class="comment"># 这是个GPU张量</span></span><br><span class="line">a0 = self.actor(s0).squeeze(<span class="number">0</span>).detach().cpu().numpy() <span class="comment"># 将输出的结果(GPU数组)拷贝回CPU才能转化</span></span><br></pre></td></tr></table></figure>

<h2 id="detach函数"><a href="#detach函数" class="headerlink" title="detach函数"></a>detach函数</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_31244453/article/details/112473947">https://blog.csdn.net/qq_31244453/article/details/112473947</a></p>
<p>返回一个新的tensor，从当前计算图中分离下来。但是仍指向原变量的存放位置，不同之处只是requirse_grad为false.得到的这个tensir永远不需要计算器梯度，不具有grad.</p>
<h2 id="某博主的DDPG"><a href="#某博主的DDPG" class="headerlink" title="某博主的DDPG"></a>某博主的DDPG</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/blanokvaffy/article/details/86232658">https://blog.csdn.net/blanokvaffy/article/details/86232658</a></p>
<h2 id="网络保存与加载"><a href="#网络保存与加载" class="headerlink" title="网络保存与加载"></a>网络保存与加载</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41680653/article/details/93768559">https://blog.csdn.net/weixin_41680653/article/details/93768559</a></p>
<h2 id="torch-mean"><a href="#torch-mean" class="headerlink" title="torch.mean()"></a>torch.mean()</h2><p> mean()函数的参数：dim=0,按行求平均值，返回的形状是（1，列数）；dim=1,按列求平均值，返回的形状是（行数，1）,默认不设置dim的时候，返回的是所有元素的平均值。</p>
<h2 id="torch的zero-grad-函数"><a href="#torch的zero-grad-函数" class="headerlink" title="torch的zero_grad()函数"></a>torch的zero_grad()函数</h2><p>调用backward()函数之前都要将梯度清零，因为如果梯度不清零，pytorch中会将上次计算的梯度和本次计算的梯度累加</p>
<h2 id="torch-tensor的view函数"><a href="#torch-tensor的view函数" class="headerlink" title="torch tensor的view函数"></a>torch tensor的view函数</h2><p>view(*args) → Tensor<br>返回一个有相同数据但大小不同的tensor。 返回的tensor必须有与原tensor相同的数据和相同数目的元素，但可以有不同的大小。</p>
<p>参数是-1的时候这个维度的长度从其他维度<strong>推算</strong></p>
<h2 id="torch-tensor的-sequeeze和unsqueeze"><a href="#torch-tensor的-sequeeze和unsqueeze" class="headerlink" title="torch tensor的 sequeeze和unsqueeze"></a>torch tensor的 sequeeze和unsqueeze</h2><p>squeeze()：</p>
<p><code>squeeze(arg)</code>表示第arg维的维度值为1，则去掉该维度。否则tensor不变。（即若<code>tensor.shape()[arg] = 1</code>，则去掉该维度）</p>
<p>unsqueeze()：</p>
<p><code>unsqueeze(arg)</code>与<code>squeeze(arg)</code>作用相反，表示在第arg维增加一个维度值为1的维度。</p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/12/08/TD3%E7%BD%91%E7%BB%9C/">TD3网络</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-12-08</time><div class="content"><h1 id="TD3网络"><a href="#TD3网络" class="headerlink" title="TD3网络"></a>TD3网络</h1><ul>
<li><p>TD3是<strong>Twin Delayed</strong> Deep Deterministic policy gradient algorithm的简称，<strong>双延迟</strong>深度确定性策略梯度</p>
<p>传统的DDPG：</p>
<p><img src="/imgs/image-20211208213332139.png" alt="image-20211208213332139"></p>
</li>
</ul>
<p>关注上图中，我们通过Critic网络估算动作的A值。一个Critic的评估可能会较高。所以我们加一个。</p>
<p><img src="/imgs/image-20211208213403185.png" alt="image-20211208213403185"></p>
<p>这就相当于我们把途中的Critic的框框，一个变为两个。</p>
<p>在目标网络中，我们估算出来的Q值会用min()函数求出较少值。以这个值作为更新的目标。</p>
<p>这个目标会更新两个网络 Critic网络_1 和 Critic网络_2。</p>
<p>你可以理解为这两个网络是完全独立，他们只是都用同一个目标进行更新。</p>
<p>剩余的就和DDPG一样了。过一段时间，把学习好的网络赋值给目标网络。</p>
<p>我们再仔细分别看Critic部分和Actor部分的学习。</p>
<h2 id="Critic部分的学习"><a href="#Critic部分的学习" class="headerlink" title="Critic部分的学习"></a>Critic部分的学习</h2><p>只有我们在计算Critic的更新目标时，我们才用target network。其中就包括了一个Policy network，用于计算A’；两个Q network ,用于计算两个Q值：Q1(A’) 和Q2(A’)。</p>
<p>Q1(A’) 和Q2(A’) 取最小值 min(Q1,Q2) 将代替DDPG的 Q(a’) 计算更新目标，也就是说： target = min(Q1,Q2) * gamma + r</p>
<p>target 将会是 Q_network_1 和 Q_network_2 两个网络的更新目标。</p>
<p>这里可能会有同学问，既然更新目标是一样的，那么为什么还需要两个网络呢?</p>
<p>虽然更新目标一样，两个网络会越来越趋近与和实际q值相同。但由于网络参数的初始值不一样，会导致计算出来的值有所不同。所以我们可以有空间选择较小的值去估算q值，避免q值被高估。</p>
<h2 id="Actor部分的学习"><a href="#Actor部分的学习" class="headerlink" title="Actor部分的学习"></a>Actor部分的学习</h2><p>我们在DDPG中说过，DDPG网络图像上就可以想象成一张布，覆盖在qtable上。当我们输入某个状态的时候，相当于这块布上的一个截面，我们我们能够看到在这个状态下的一条曲线。</p>
<p>而actor的任务，就是用梯度上升的方法，寻着这条线的最高点。</p>
<p>对于actor来说，其实并不在乎Q值是否会被高估，他的任务只是不断做梯度上升，寻找这条最大的Q值。随着更新的进行Q1和Q2两个网络，将会变得越来越像。所以用Q1还是Q2，还是两者都用，对于actor的问题不大。</p>
<h2 id="Delayed-延迟"><a href="#Delayed-延迟" class="headerlink" title="Delayed - 延迟"></a>Delayed - 延迟</h2><p>这里说的Dalayed ，是actor更新的delay。也就是说相对于critic可以更新多次后，actor再进行更新。</p>
<p>为什么要这样做呢？</p>
<p>还是回到我们qnet拟合出来的那块”布”上。</p>
<p>qnet在学习过程中，我们的q值是不断变化的，也就是说这块布是不断变形的。所以要寻着最高点的任务有时候就挺难为为的actor了。</p>
<p>可以想象，本来是最高点的，当actor好不容易去到最高点；q值更新了，这并不是最高点。这时候actor只能转头再继续寻找新的最高点。更坏的情况可能是actor被困在次高点，没有找到正确的最高点。</p>
<p>所以我们可以把Critic的更新频率，调的比Actor要高一点。让critic更加确定，actor再行动。</p>
<h2 id="target-policy-smoothing-regularization"><a href="#target-policy-smoothing-regularization" class="headerlink" title="target policy smoothing regularization"></a>target policy smoothing regularization</h2><p>TD3中，价值函数的更新目标每次都在action上加一个小扰动，这个操作就是target policy smoothing regularization</p>
<p>为什么要这样呢？</p>
<p>我们可以再次回到我们关于“布”的想象。</p>
<p>在DDPG中，计算target的时候，我们输入时s_和a_，获得q，也就是这块布上的一点A。通过估算target估算另外一点s，a，也就是布上的另外一点B的Q值。<br><img src="/imgs/image-20211208213605697.png" alt="image-20211208213605697"></p>
<p>在TD3中，计算target时候，输入s_到actor输出a后，给a加上噪音，让a在一定范围内随机。这又什么好处呢。</p>
<p>好处就是，当更新多次的时候，就相当于用A点附近的一小部分范围（准确来说是在s_这条线上的一定范围）的去估算B，这样可以让B点的估计更准确，更健壮。</p>
<p><img src="/imgs/image-20211208213632464.png" alt="image-20211208213632464"></p>
<ul>
<li>这注意区分三个地方：</li>
</ul>
<p>​    在跑游戏的时候，我们同样加上了了noise。这个时候的noise是为了更充分地开发整个游戏空间。<br>​    计算target的时候，actor加上noise，是为了预估更准确，网络更有健壮性。<br>​    更新actor的时候，我们不需要加上noise，这里是希望actor能够寻着最大值。加上noise并没有任何意义。</p>
<h2 id="下面附上源代码"><a href="#下面附上源代码" class="headerlink" title="下面附上源代码"></a>下面附上源代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;迁移到了GPU上进行训练&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Actor</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Actor, self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(input_size, hidden_size)</span><br><span class="line">        self.linear2 = nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        self.linear3 = nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        self.linear4 = nn.Linear(hidden_size, output_size)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, s</span>):</span></span><br><span class="line">        x = F.relu(self.linear1(s))</span><br><span class="line">        x = F.relu(self.linear2(x))</span><br><span class="line">        x = F.relu(self.linear3(x))</span><br><span class="line">        x = torch.tanh(self.linear4(x))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Critic</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.linear1 = nn.Linear(input_size, hidden_size)</span><br><span class="line">        self.linear2 = nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        self.linear3 = nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        self.linear4 = nn.Linear(hidden_size, output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, s, a</span>):</span></span><br><span class="line">        x = torch.cat([s, a], <span class="number">1</span>)</span><br><span class="line">        x = F.relu(self.linear1(x))</span><br><span class="line">        x = F.relu(self.linear2(x))</span><br><span class="line">        x = F.relu(self.linear3(x))</span><br><span class="line">        x = self.linear4(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="keyword">for</span> key, value <span class="keyword">in</span> kwargs.items():</span><br><span class="line">            <span class="built_in">setattr</span>(self, key, value)</span><br><span class="line"></span><br><span class="line">        s_dim = self.env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line">        a_dim = self.env.action_space.shape[<span class="number">0</span>]</span><br><span class="line">        fileList = os.listdir(<span class="string">&#x27;nets/&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&quot;actor.pkl&quot;</span> <span class="keyword">in</span> fileList :</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Trained nets found!&quot;</span>)</span><br><span class="line"></span><br><span class="line">            self.actor = torch.load(<span class="string">&#x27;nets/actor.pkl&#x27;</span>)</span><br><span class="line">            self.actor_target = torch.load(<span class="string">&#x27;nets/actor_target.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            self.critic1 = torch.load(<span class="string">&#x27;nets/critic1.pkl&#x27;</span>)</span><br><span class="line">            self.critic_target1 = torch.load(<span class="string">&#x27;nets/critic_target1.pkl&#x27;</span>)</span><br><span class="line">            self.critic2 = torch.load(<span class="string">&#x27;nets/critic2.pkl&#x27;</span>)</span><br><span class="line">            self.critic_target2 = torch.load(<span class="string">&#x27;nets/critic_target2.pkl&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Trained nets not found!&quot;</span>)</span><br><span class="line"></span><br><span class="line">            self.actor = Actor(s_dim, <span class="number">256</span>, a_dim).cuda()</span><br><span class="line">            self.actor_target = Actor(s_dim, <span class="number">256</span>, a_dim).cuda()</span><br><span class="line"></span><br><span class="line">            self.critic1 = Critic(s_dim + a_dim, <span class="number">256</span>, <span class="number">1</span>).cuda()   <span class="comment"># 此处修改了critic的输出维度恒为1</span></span><br><span class="line">            self.critic_target1 = Critic(s_dim + a_dim, <span class="number">256</span>, <span class="number">1</span>).cuda()  <span class="comment"># 此处修改了critic的输出维度恒为1</span></span><br><span class="line">            self.critic2 = Critic(s_dim + a_dim, <span class="number">256</span>, <span class="number">1</span>).cuda()  <span class="comment"># 此处修改了critic的输出维度恒为1</span></span><br><span class="line">            self.critic_target2 = Critic(s_dim + a_dim, <span class="number">256</span>, <span class="number">1</span>).cuda()  <span class="comment"># 此处修改了critic的输出维度恒为1</span></span><br><span class="line">            <span class="comment"># 假如没找到存在的网络的话，初始化target网络</span></span><br><span class="line">            self.actor_target.load_state_dict(self.actor.state_dict())</span><br><span class="line"></span><br><span class="line">            self.critic_target1.load_state_dict(self.critic1.state_dict())</span><br><span class="line">            self.critic_target2.load_state_dict(self.critic2.state_dict())</span><br><span class="line"></span><br><span class="line">        self.actor_optim = optim.Adam(self.actor.parameters(), lr=self.actor_lr)</span><br><span class="line">        self.critic_optim1 = optim.Adam(self.critic1.parameters(), lr=self.critic_lr)</span><br><span class="line">        self.critic_optim2 = optim.Adam(self.critic2.parameters(), lr=self.critic_lr)</span><br><span class="line">        self.buffer = []</span><br><span class="line">        self.updateCnt = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">act</span>(<span class="params">self, s0</span>):</span></span><br><span class="line">        s0 = torch.tensor(s0, dtype=torch.<span class="built_in">float</span>).cuda().unsqueeze(<span class="number">0</span>).cuda()</span><br><span class="line">        a0 = self.actor(s0).squeeze(<span class="number">0</span>).detach().cpu().numpy()</span><br><span class="line">        <span class="keyword">return</span> a0</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">put</span>(<span class="params">self, *transition</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.buffer) == self.capacity:</span><br><span class="line">            self.buffer.pop(<span class="number">0</span>)</span><br><span class="line">        self.buffer.append(transition)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">learn</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.buffer) &lt; self.batch_size:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        samples = random.sample(self.buffer, self.batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        s0, a0, r1, s1 = <span class="built_in">zip</span>(*samples)</span><br><span class="line"></span><br><span class="line">        s0 = torch.tensor(s0, dtype=torch.<span class="built_in">float</span>).cuda()</span><br><span class="line">        a0 = torch.tensor(a0, dtype=torch.<span class="built_in">float</span>).cuda()</span><br><span class="line">        r1 = torch.tensor(r1, dtype=torch.<span class="built_in">float</span>).view(self.batch_size, -<span class="number">1</span>).cuda()</span><br><span class="line">        s1 = torch.tensor(s1, dtype=torch.<span class="built_in">float</span>).cuda()</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">critic_learn</span>():</span></span><br><span class="line">            a1 = self.actor_target(s1).detach()</span><br><span class="line">            y_true = r1 + self.gamma * torch.<span class="built_in">min</span>(self.critic_target1(s1, a1), self.critic_target1(s1, a1)).detach()</span><br><span class="line">            <span class="comment"># 更新网咯1</span></span><br><span class="line">            y_pred1 = self.critic1(s0, a0)</span><br><span class="line">            loss_fn = nn.MSELoss()</span><br><span class="line">            loss = loss_fn(y_pred1, y_true)</span><br><span class="line">            self.critic_optim1.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            self.critic_optim1.step()</span><br><span class="line">            <span class="comment"># 更新网络2</span></span><br><span class="line">            y_pred2 = self.critic2(s0, a0)</span><br><span class="line">            loss_fn = nn.MSELoss()</span><br><span class="line">            loss = loss_fn(y_pred2, y_true)</span><br><span class="line">            self.critic_optim2.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            self.critic_optim2.step()</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">actor_learn</span>():</span></span><br><span class="line">            <span class="comment"># 此处update actor网络同样从两个critic网络中选择一个较小的</span></span><br><span class="line">            loss = -torch.mean(torch.<span class="built_in">min</span>(self.critic1(s0, self.actor(s0)), self.critic2(s0, self.actor(s0))))</span><br><span class="line">            self.actor_optim.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            self.actor_optim.step()</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">soft_update</span>(<span class="params">net_target, net, tau</span>):</span></span><br><span class="line">            <span class="keyword">for</span> target_param, param <span class="keyword">in</span> <span class="built_in">zip</span>(net_target.parameters(), net.parameters()):</span><br><span class="line">                target_param.data.copy_(target_param.data * (<span class="number">1.0</span> - tau) + param.data * tau)</span><br><span class="line"></span><br><span class="line">        critic_learn()</span><br><span class="line">        soft_update(self.critic_target1, self.critic1, self.tau)</span><br><span class="line">        soft_update(self.critic_target2, self.critic2, self.tau)</span><br><span class="line">        self.updateCnt += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 到达更新频率的时候才更新actor</span></span><br><span class="line">        <span class="keyword">if</span>((self.updateCnt % self.update_interval) == <span class="number">0</span>):</span><br><span class="line">            actor_learn()</span><br><span class="line">            soft_update(self.actor_target, self.actor, self.tau)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span>(<span class="params">self</span>):</span></span><br><span class="line">        torch.save(self.actor, <span class="string">&#x27;nets/actor.pkl&#x27;</span>)</span><br><span class="line">        torch.save(self.actor_target, <span class="string">&#x27;nets/actor_target.pkl&#x27;</span>)</span><br><span class="line">        torch.save(self.critic1, <span class="string">&#x27;nets/critic1.pkl&#x27;</span>)</span><br><span class="line">        torch.save(self.critic_target1, <span class="string">&#x27;nets/critic_target1.pkl&#x27;</span>)</span><br><span class="line">        torch.save(self.critic2, <span class="string">&#x27;nets/critic2.pkl&#x27;</span>)</span><br><span class="line">        torch.save(self.critic_target2, <span class="string">&#x27;nets/critic_target2.pkl&#x27;</span>)</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&#x27;Pendulum-v1&#x27;</span>)</span><br><span class="line">env.reset()</span><br><span class="line">env.render()</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;env&#x27;</span>: env,</span><br><span class="line">    <span class="string">&#x27;gamma&#x27;</span>: <span class="number">0.99</span>,</span><br><span class="line">    <span class="string">&#x27;actor_lr&#x27;</span>: <span class="number">0.001</span>,</span><br><span class="line">    <span class="string">&#x27;critic_lr&#x27;</span>: <span class="number">0.0013</span>,</span><br><span class="line">    <span class="string">&#x27;tau&#x27;</span>: <span class="number">0.02</span>,</span><br><span class="line">    <span class="string">&#x27;capacity&#x27;</span>: <span class="number">5000</span>,</span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>: <span class="number">32</span>,</span><br><span class="line">    <span class="string">&#x27;update_interval&#x27;</span>: <span class="number">3</span>,</span><br><span class="line">&#125;</span><br><span class="line">EPOCH_NUM = <span class="number">200</span></span><br><span class="line">agent = Agent(**params)</span><br><span class="line">FLAG = <span class="literal">False</span></span><br><span class="line">rewardList = []</span><br><span class="line"><span class="comment"># INTCOEFF = 0.001</span></span><br><span class="line">integral = <span class="number">0</span></span><br><span class="line"><span class="comment"># INTCOEFF = 0.0</span></span><br><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH_NUM):</span><br><span class="line">    s0 = env.reset()</span><br><span class="line">    episode_reward = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span>(episode%<span class="number">20</span> == <span class="number">0</span>):</span><br><span class="line">        flag = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        flag = <span class="literal">False</span></span><br><span class="line">    integral = <span class="number">0</span></span><br><span class="line">    INTCOEFF = (episode/EPOCH_NUM)**<span class="number">2</span>*<span class="number">0.005</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">        <span class="keyword">if</span>(flag):</span><br><span class="line">            env.render()</span><br><span class="line">        a0 = agent.act(s0)</span><br><span class="line">        s1, r1, done, _ = env.step(a0)</span><br><span class="line">        integral += r1*INTCOEFF</span><br><span class="line">        agent.put(s0, a0, r1+integral, s1)</span><br><span class="line"></span><br><span class="line">        episode_reward += r1</span><br><span class="line">        s0 = s1</span><br><span class="line"></span><br><span class="line">        agent.learn()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(episode, <span class="string">&#x27;: &#x27;</span>, episode_reward)</span><br><span class="line">    rewardList.append(episode_reward)</span><br><span class="line">pltX = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(EPOCH_NUM)]</span><br><span class="line">plt.plot(pltX, rewardList)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># agent.save()</span></span><br></pre></td></tr></table></figure>

<p>详见<a target="_blank" rel="noopener" href="https://gitee.com/frankzhang0219/ddpg_try">DDPG_Try: DDPG尝试集 (gitee.com)</a></p>
</div><hr></div><div class="recent-post-item article-container"><a class="article-title" href="/2021/12/08/%E5%AE%89%E8%A3%85NodeJS%E4%BB%A5%E5%8F%8Ahexo/">安装NodeJS以及hexo</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-12-08</time><div class="content"><h1 id="安装nodeJS以及hexo框架的踩坑笔记"><a href="#安装nodeJS以及hexo框架的踩坑笔记" class="headerlink" title="安装nodeJS以及hexo框架的踩坑笔记"></a>安装nodeJS以及hexo框架的踩坑笔记</h1><ul>
<li>版本问题：<br>一开始使用的是最新版本的NodeJS然后安装完成Hexo之后发现不能在控制台使用hexo init blog，显示hexo不是命令。<br>然后使用HodeJS12.0.0，自带npm，安装完成（注意不要使用13等版本，npm不是支持每个nodeJS版本）</li>
<li>假如直接<code>git clone</code>hexo的仓库报错的话可能是换设备之后 <strong>无法通用</strong> ，还是需要在 <strong>原设备</strong> 上才能打的开<img src="/imgs/image-20211214003223898.png" alt="image-20211214003223898"></li>
<li>假如重装hexo失败的话，删除图中文件夹下的hexo有关的所有内容<img src="/imgs/image-20211208201712068.png" alt="image-20211208201712068"></li>
<li>安装完成之后在控制台输入以下代码，假如没有报错而且能够正确显示版本号的话说明安装成功<br><code>node -v</code><code>npm -v</code></li>
</ul>
<ul>
<li>另注意，假如安装完nodeJS之后，输入<code>node -v</code>得到的版本和安装的 <strong>不同</strong> ，此时需要将NodeJS卸载然后安装在 <strong>默认的安装位置</strong> 。注意，AMD CPU的计算机下载x86的，Intel的下载x64的msi安装文件</li>
</ul>
<ul>
<li><p>插入图片问题<br>有时候插入的图片会加载失败比如显示为 <img src="/imgs/2.png" alt="2">，此时可以通过以下几步解决问题</p>
</li>
<li><p>首先，在source目录下新建一个文件夹比如imgs</p>
<p><img src="/imgs/image-20211208204226807.png" alt="image-20211208204226807"></p>
<p>然后把typora的根目录设置为source</p>
<p><img src="/imgs/image-20211208204322540.png" alt="image-20211208204322540"></p>
<p>然后设置将图片复制到imgs下</p>
<p><img src="/imgs/image-20211208204358165.png" alt="image-20211208204358165"></p>
<p>配置好上述功能之后，直接把用到的图片从剪贴板复制到typora即可</p>
<p><strong>不要安装</strong>网上博客说的插件，安装之后可能无法启动网站</p>
<p>也<strong>不需要</strong>在config中开启每新建一篇博客就新建一个文件夹之类的选项，反而更麻烦</p>
</li>
<li><p>修改主题<br>从github将主题项目 <code>git clone</code>到blogs的 <code>\themes\&lt;主题名称&gt;</code>文件夹，然后将总目录下的 <code>_config.yml</code>中的 <code>theme</code>更换为&lt;主题名称&gt;<br>假如无法clone的话，可以将项目打包下载然后解压到同样的文件夹中即可</p>
</li>
<li><p>设置个人信息如下图<br><img src="/imgs/3.png" alt="3"></p>
</li>
<li><p>此外，在主题的配置文件中（也是 <code>_config.yml</code>）也有一些可以修改的地方，可以自己动手改一改</p>
</li>
<li><p>在推送网站之前需要安装一个git配合插件<br><code>npm install hexo-deployer-git --save</code></p>
</li>
</ul>
<h2 id="更换设备"><a href="#更换设备" class="headerlink" title="更换设备"></a>更换设备</h2><ul>
<li>注意更换设备的时候<strong>不能</strong>简单的<code>git clone</code>，这样的话是无法打开的。只能自己在本地新建一个博客，然后将<code>source</code>和<code>themes</code>以及<code>config</code>文件和文件夹都 <strong>复制进去</strong></li>
</ul>
<h2 id="部署的地址"><a href="#部署的地址" class="headerlink" title="部署的地址"></a>部署的地址</h2><ul>
<li>部署的时候 <strong><u>不能使用</u></strong> 网页上的教程repo地址如图</li>
</ul>
<p><img src="/imgs/image-20211209101305939.png" alt="image-20211209101305939"></p>
<p>需要直接写成自己的<strong>仓库地址+.git</strong>，比如</p>
<p><img src="/imgs/image-20211209101336927.png" alt="image-20211209101336927"></p>
<ul>
<li>此外，相关的 <strong>url和root</strong> 配置如下;</li>
</ul>
<p><img src="/imgs/image-20211209101427432.png" alt="image-20211209101427432"></p>
<p>即可正常显示</p>
<h2 id="向服务器提交过程"><a href="#向服务器提交过程" class="headerlink" title="向服务器提交过程"></a>向服务器提交过程</h2><p><code>hexo clean</code></p>
<p><code>hexo g</code></p>
<p><code>hexo d</code></p>
<h2 id="详细入门网站"><a href="#详细入门网站" class="headerlink" title="详细入门网站"></a>详细入门网站</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26625249">https://zhuanlan.zhihu.com/p/26625249</a></p>
<h2 id="gitee网站教程"><a href="#gitee网站教程" class="headerlink" title="gitee网站教程"></a>gitee网站教程</h2><p><a target="_blank" rel="noopener" href="https://gitee.com/help/articles/4136#article-header0">https://gitee.com/help/articles/4136#article-header0</a></p>
<h2 id="vscode-markdown插件（相对方便，但是不支持图片复制，建议还是用typora）"><a href="#vscode-markdown插件（相对方便，但是不支持图片复制，建议还是用typora）" class="headerlink" title="vscode markdown插件（相对方便，但是不支持图片复制，建议还是用typora）"></a>vscode markdown插件（相对方便，但是不支持图片复制，建议还是用typora）</h2><p>office viewer</p>
<p><img src="/imgs/4.png" alt="4"></p>
<hr>
</div><hr></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/8/"><i class="fa fa-chevron-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2021 - 2022 By FrankZhang</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script></body></html>